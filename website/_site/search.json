[
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Security Incidents Data Cleaning",
    "section": "",
    "text": "This document outlines the data cleaning process for the security incidents dataset. We‚Äôll examine the data structure, identify and address missing values, and prepare the dataset for analysis of broad trends in aid worker incidents by country, year, and organization type.\nThe cleaning process follows a systematic approach:\n\nLoading and initial inspection\nHandling missing values\nDuplicate detection and removal\nData type optimization\nOutlier analysis and handling\nGeographic data validation\nFinal dataset preparation"
  },
  {
    "objectID": "data_cleaning.html#step-1-exploring-what-needs-cleaning",
    "href": "data_cleaning.html#step-1-exploring-what-needs-cleaning",
    "title": "Data Cleaning",
    "section": "Step 1: Exploring What Needs Cleaning",
    "text": "Step 1: Exploring What Needs Cleaning\nBefore cleaning the dataset, let‚Äôs inspect the structure, missing values, and data types to understand what issues we need to address.\n\n\nCode\ndf.shape\n\n\n(4337, 41)\n\n\nThere are 41 columns and 4337 rows.\n\n\nCode\ndf.dtypes\n\n\nincident_id                   int64\nyear                          int64\nmonth                       float64\nday                         float64\ncountry_code                 object\ncountry                      object\nregion                       object\ndistrict                     object\ncity                         object\nun                            int64\ningo                          int64\nicrc                        float64\nnrcs_and_ifrc               float64\nnngo                        float64\nother                         int64\nnationals_killed              int64\nnationals_wounded             int64\nnationals_kidnapped           int64\ntotal_nationals               int64\ninternationals_killed         int64\ninternationals_wounded        int64\ninternationals_kidnapped      int64\ntotal_internationals          int64\ntotal_killed                  int64\ntotal_wounded                 int64\ntotal_kidnapped               int64\ntotal_affected                int64\ngender_male                   int64\ngender_female                 int64\ngender_unknown                int64\nmeans_of_attack              object\nattack_context               object\nlocation                     object\nlatitude                    float64\nlongitude                   float64\nmotive                       object\nactor_type                   object\nactor_name                   object\ndetails                      object\nverified                     object\nsource                       object\ndtype: object\n\n\nMost data types in this dataset are appropriate. Columns like incident_id and year are correctly stored as integers, while text-based fields such as country, region, motive, and actor_type are stored as objects, which is suitable for categorical or descriptive information. Count-based columns like un, ingo, other, total_, and gender_ are also properly stored as integers. However, month and day are currently floats due to missing values, but should be converted to integers after filling. Similarly, icrc, nrcs_and_ifrc, and nngo are counts but stored as floats, so converting them to integers will improve consistency. Latitude and longitude are correctly stored as floats since they contain decimal values. Overall, the data types are mostly correct, with just a few minor adjustments needed.\n\nüõ† Optional Data Type Fixes\n\n\n\n\n\n\n\n\n\nColumn\nCurrent\nSuggested\nReason\n\n\n\n\nmonth, day\nfloat64\nint64\nWhole numbers; convert after filling missing values.\n\n\nicrc\nfloat64\nint64\nCount; cleaner as integer.\n\n\nnrcs_and_ifrc\nfloat64\nint64\nCount; cleaner as integer.\n\n\nnngo\nfloat64\nint64\nCount; cleaner as integer.\n\n\n\n\n\nCode\n# Calculate percent of missing values per column\nmissing_percent = (df.isna().sum() / len(df)) * 100\nmissing_percent = missing_percent[missing_percent &gt; 0].sort_values(ascending=True)\n\n# Re-plot with smaller dimensions\nplt.figure(figsize=(7, 4))  # reduced size\nmissing_percent.plot(kind='barh')\nplt.title(\"Percentage of Missing Values per Column\")\nplt.xlabel(\"Percent Missing (%)\")\nplt.tight_layout()\nplt.grid(axis='x', linestyle='--', alpha=0.5)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nüîç Step 2: Visualizing Missing Data\nTo better understand which variables have missing values, we created a horizontal bar plot showing the percentage of missing values per column. This makes it easier to identify which variables might require imputation, removal, or further attention.\nAs seen below, City, District, Day, and Region have the highest proportions of missing data, with City missing in over 20% of records.\n\n\nüóë Step 3: Dropping Low-Value or Redundant Columns\nWhile some columns like Day, City, District, and Region provide detailed location and date information, they have a high proportion of missing values and offer limited analytical value for our high-level analysis.\nSince our goal is to examine broad trends in aid worker incidents by country, year, and organization type, we decided to remove these granular fields:\n\nDay and Month: Year is sufficient for time trends.\nDistrict, City, Region: Country-level insights are the focus."
  },
  {
    "objectID": "index.html#about-this-project",
    "href": "index.html#about-this-project",
    "title": "Humanitarian Aid Under Fire",
    "section": "About This Project",
    "text": "About This Project"
  },
  {
    "objectID": "index.html#about-this-dataset-aid-worker-security-incidents",
    "href": "index.html#about-this-dataset-aid-worker-security-incidents",
    "title": "Humanitarian Aid Under Fire",
    "section": "üîê About This Dataset: Aid Worker Security Incidents",
    "text": "üîê About This Dataset: Aid Worker Security Incidents\nThis dataset security_incidents.csv comes from the Aid Worker Security Database (AWSD), a global resource that tracks major incidents of violence against humanitarian aid workers. Maintained by Humanitarian Outcomes, the AWSD is a leading source used by researchers, NGOs, and policy makers to understand risks faced by aid personnel in the field."
  },
  {
    "objectID": "index.html#whats-included-in-the-data",
    "href": "index.html#whats-included-in-the-data",
    "title": "Humanitarian Aid Under Fire",
    "section": "üìä What‚Äôs Included in the Data?",
    "text": "üìä What‚Äôs Included in the Data?\nEach row in this dataset represents a documented security incident involving humanitarian organizations. Key information includes:\n\nDate and Location: Year, Month, Country, Region, District, City, Coordinates\n\nOrganization Type: UN, INGOs, NNGOs, ICRC/IFRC, Other aid actors\n\nIncident Impact: Number of nationals and internationals killed, wounded, or kidnapped\n\nAttack Details: Type of attack (e.g., shooting, kidnapping), context (e.g., ambush, raid), and actor information\n\nAdditional Fields: Motive, actor type and name, description, and data verification status"
  },
  {
    "objectID": "index.html#key-questions",
    "href": "index.html#key-questions",
    "title": "Humanitarian Aid Under Fire",
    "section": "Key Questions",
    "text": "Key Questions\n\nmermaid diagram"
  },
  {
    "objectID": "index.html#why-this-matters",
    "href": "index.html#why-this-matters",
    "title": "Humanitarian Aid Under Fire",
    "section": "üåç Why This Matters",
    "text": "üåç Why This Matters\nThe AWSD is the only comprehensive global database that documents targeted violence against aid workers. It distinguishes between national and international staff, allowing deeper insights into the differential risks they face. The database supports humanitarian access planning, risk assessment, and evidence-based security policy.\n\nüìÇ Explore More\n\nVisualizations\nMethods\nData Summary\nDownload the Dataset"
  },
  {
    "objectID": "index.html#source-and-access",
    "href": "index.html#source-and-access",
    "title": "Humanitarian Aid Under Fire",
    "section": "üì• Source and Access",
    "text": "üì• Source and Access\nThis dataset was downloaded directly from: üëâ Aid Worker Security Database (AWSD)\nPlease cite as:\nHumanitarian Outcomes. Aid Worker Security Database (AWSD). https://aidworkersecurity.org"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "data_cleaning.html#missing-values",
    "href": "data_cleaning.html#missing-values",
    "title": "Data Cleaning",
    "section": "Missing Values",
    "text": "Missing Values\nTo better understand which variables have missing values, we created a horizontal bar plot showing the percentage of missing values per column. This makes it easier to identify which variables might require imputation, removal, or further attention.\nAs seen below, City, District, Day, and Region have the highest proportions of missing data, with City missing in over 20% of records.\n\n\nCode\n# Calculate percent of missing values per column\nmissing_percent = (df.isna().sum() / len(df)) * 100\nmissing_percent = missing_percent[missing_percent &gt; 0].sort_values(ascending=True)\n\n# Re-plot with smaller dimensions\nplt.figure(figsize=(7, 4))  # reduced size\nmissing_percent.plot(kind='barh')\nplt.title(\"Percentage of Missing Values per Column\")\nplt.xlabel(\"Percent Missing (%)\")\nplt.tight_layout()\nplt.grid(axis='x', linestyle='--', alpha=0.5)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nWhile some columns like Day, Month, City, District, and Region provide detailed location and date information, they have a high proportion of missing values and offer limited analytical value for our analysis.\nSince our goal is to examine broad trends in aid worker incidents by country, year, and organization type, we decided to remove these granular fields:\n\nDay and Month: Year is sufficient for time trends.\nDistrict, City, Region, Country Code: Country-level insights are the focus.\n\n\n\nCode\ncolumns_to_drop = ['day', 'month', 'district', 'city', 'region', 'country_code']\ndf.drop(columns=columns_to_drop, inplace=True)\n\n\n\n\nCode\n# Calculate percent of missing values per column\nmissing_percent = (df.isna().sum() / len(df)) * 100\nmissing_percent = missing_percent[missing_percent &gt; 0].sort_values(ascending=True)\n\n# Re-plot with smaller dimensions\nplt.figure(figsize=(7, 4))  # reduced size\nmissing_percent.plot(kind='barh')\nplt.title(\"Percentage of Missing Values per Column\")\nplt.xlabel(\"Percent Missing (%)\")\nplt.tight_layout()\nplt.grid(axis='x', linestyle='--', alpha=0.5)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nSince the highest missing value is less than 0.5%, and the remaining missing data is sparse, likely random, and not concentrated in critical columns, no imputation is necessary. We chose to leave these values as-is (NaN), as most analysis tools handle them gracefully and imputing could introduce unnecessary bias."
  },
  {
    "objectID": "data_cleaning.html#exploring-what-needs-cleaning",
    "href": "data_cleaning.html#exploring-what-needs-cleaning",
    "title": "Data Cleaning",
    "section": "Exploring What Needs Cleaning",
    "text": "Exploring What Needs Cleaning\nBefore cleaning the dataset, let‚Äôs inspect the structure, missing values, and data types to understand what issues we need to address.\n\n\nCode\nrows, cols = df.shape\nprint(f\"The dataset contains {rows} rows and {cols} columns.\")\n\n\nThe dataset contains 4337 rows and 35 columns.\n\n\nNext, we explore data types. Most data types in this dataset are appropriate. Columns like incident_id and year are correctly stored as integers, while text-based fields such as country, region, motive, and actor_type are stored as objects, which is suitable for categorical or descriptive information. Count-based columns like un, ingo, other, total_, and gender_ are also properly stored as integers. However, month and day are currently floats due to missing values, but should be converted to integers after filling. Similarly, icrc, nrcs_and_ifrc, and nngo are counts but stored as floats, so converting them to integers will improve consistency. Latitude and longitude are correctly stored as floats since they contain decimal values. Overall, the data types are mostly correct, with just a few minor adjustments needed.\n\nüõ† Optional Data Type Fixes\n\n\n\n\n\n\n\n\n\nColumn\nCurrent\nSuggested\nReason\n\n\n\n\nmonth, day\nfloat64\nint64\nWhole numbers; convert after filling missing values.\n\n\nicrc\nfloat64\nint64\nCount; cleaner as integer.\n\n\nnrcs_and_ifrc\nfloat64\nint64\nCount; cleaner as integer.\n\n\nnngo\nfloat64\nint64\nCount; cleaner as integer."
  },
  {
    "objectID": "data_cleaning.html#introduction",
    "href": "data_cleaning.html#introduction",
    "title": "Security Incidents Data Cleaning",
    "section": "",
    "text": "This document outlines the data cleaning process for the security incidents dataset. We‚Äôll examine the data structure, identify and address missing values, and prepare the dataset for analysis of broad trends in aid worker incidents by country, year, and organization type.\nThe cleaning process follows a systematic approach:\n\nLoading and initial inspection\nHandling missing values\nDuplicate detection and removal\nData type optimization\nOutlier analysis and handling\nGeographic data validation\nFinal dataset preparation"
  },
  {
    "objectID": "data_cleaning.html#loading-and-initial-inspection",
    "href": "data_cleaning.html#loading-and-initial-inspection",
    "title": "Security Incidents Data Cleaning",
    "section": "2. Loading and Initial Inspection",
    "text": "2. Loading and Initial Inspection\nFirst, let‚Äôs load the dataset and standardize the column names for consistency.\n\n\nCode\n# Import required libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Load the security incidents dataset\ndf = pd.read_csv(\"data/security_incidents.csv\")\n\n# Standardize column names (lowercase, replace spaces with underscores)\ndf.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n\n# Display the first few rows\ndf.head()\n\n\n\n\n\n\n\n\n\nincident_id\nyear\nmonth\nday\ncountry_code\ncountry\nregion\ndistrict\ncity\nun\n...\nattack_context\nlocation\nlatitude\nlongitude\nmotive\nactor_type\nactor_name\ndetails\nverified\nsource\n\n\n\n\n0\n1\n1997\n1.0\nNaN\nKH\nCambodia\nBanteay Meanchey\nNaN\nNaN\n0\n...\nUnknown\nUnknown\n14.070929\n103.099916\nUnknown\nUnknown\nUnknown\n1 ICRC national staff killed while working in ...\nArchived\nArchived\n\n\n1\n2\n1997\n1.0\nNaN\nRW\nRwanda\nNorthern\nMusanze\nRuhengeri\n0\n...\nRaid\nOffice/compound\n-1.499840\n29.634970\nUnknown\nUnknown\nUnknown\n3 INGO international (Spanish) staff killed, 1...\nArchived\nArchived\n\n\n2\n3\n1997\n2.0\nNaN\nTJ\nTajikistan\nNaN\nNaN\nNaN\n4\n...\nUnknown\nUnknown\n38.628173\n70.815654\nNaN\nUnknown\nUnknown\n3 UN national staff, 1 UN international (Niger...\nArchived\nArchived\n\n\n3\n4\n1997\n2.0\nNaN\nSO\nSomalia\nLower Juba\nKismayo\nKismayo\n0\n...\nUnknown\nUnknown\n-0.358216\n42.545087\nPolitical\nNon-state armed group: Regional\nAl-Itihaad al-Islamiya\n1 INGO international staff killed by Al ittiha...\nArchived\nArchived\n\n\n4\n5\n1997\n2.0\n14.0\nRW\nRwanda\nKigali\nKigali\nKigali\n1\n...\nIndividual attack\nUnknown\n-1.950851\n30.061508\nPolitical\nUnknown\nUnknown\n1 UN national staff shot and killed in Kigali ...\nArchived\nArchived\n\n\n\n\n5 rows √ó 41 columns\n\n\n\n\n\nCode\n# Get basic dataset information\nrows, cols = df.shape\nprint(f\"The dataset contains {rows} rows and {cols} columns.\")\n\n\nThe dataset contains 4337 rows and 41 columns."
  },
  {
    "objectID": "data_cleaning.html#handling-missing-values",
    "href": "data_cleaning.html#handling-missing-values",
    "title": "Security Incidents Data Cleaning",
    "section": "3. Handling Missing Values",
    "text": "3. Handling Missing Values\nTo better understand which variables have missing values, we‚Äôll create a visualization showing the percentage of missing values per column.\n\n\nCode\n# Calculate percent of missing values per column\nmissing_percent = (df.isna().sum() / len(df)) * 100\nmissing_percent = missing_percent[missing_percent &gt; 0].sort_values(ascending=True)\n\n# Create a horizontal bar plot of missing values\nplt.figure(figsize=(7, 4))\nmissing_percent.plot(kind='barh')\nplt.title(\"Percentage of Missing Values per Column\")\nplt.xlabel(\"Percent Missing (%)\")\nplt.tight_layout()\nplt.grid(axis='x', linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3.1 Strategy for Missing Values\nBased on our exploration, we have identified several columns with high proportions of missing values:\n\nCity (over 20% missing)\nDistrict, Day, and Region (significant proportions missing)\n\nSince our analysis focus is on broad trends by country, year, and organization type, we will:\n\nRemove granular fields with limited analytical value for our specific goals\nLeave the remaining minimal missing values as-is, as they‚Äôre sparse and likely random\n\n\n\nCode\n# Remove columns with limited analytical value for our specific analysis goals\ncolumns_to_drop = ['day', 'month', 'district', 'city', 'region', 'country_code', 'incident_id']\ndf.drop(columns=columns_to_drop, inplace=True)\n\n# Recalculate missing values after dropping columns\nmissing_percent = (df.isna().sum() / len(df)) * 100\nmissing_percent = missing_percent[missing_percent &gt; 0].sort_values(ascending=True)\n\n# Visualize the remaining missing values\nplt.figure(figsize=(7, 4))\nmissing_percent.plot(kind='barh')\nplt.title(\"Percentage of Missing Values After Column Removal\")\nplt.xlabel(\"Percent Missing (%)\")\nplt.tight_layout()\nplt.grid(axis='x', linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.2 Missing Values Decision\nSince the highest missing value percentage is now less than 0.5%, and the remaining missing data is sparse, likely random, and not concentrated in critical columns, we‚Äôve decided:\n\nNo imputation is necessary\nLeave these values as-is (NaN), as most analysis tools handle them gracefully\nImputing could introduce unnecessary bias given the small percentage"
  },
  {
    "objectID": "data_cleaning.html#data-type-considerations",
    "href": "data_cleaning.html#data-type-considerations",
    "title": "Security Incidents Data Cleaning",
    "section": "Data Type Considerations",
    "text": "Data Type Considerations\nMost data types in this dataset are appropriate, but there are a few minor adjustments that could be made for consistency:\n\n\nCode\n# Display current data types\ndf.dtypes\n\n\nincident_id                   int64\nyear                          int64\ncountry                      object\nun                            int64\ningo                          int64\nicrc                        float64\nnrcs_and_ifrc               float64\nnngo                        float64\nother                         int64\nnationals_killed              int64\nnationals_wounded             int64\nnationals_kidnapped           int64\ntotal_nationals               int64\ninternationals_killed         int64\ninternationals_wounded        int64\ninternationals_kidnapped      int64\ntotal_internationals          int64\ntotal_killed                  int64\ntotal_wounded                 int64\ntotal_kidnapped               int64\ntotal_affected                int64\ngender_male                   int64\ngender_female                 int64\ngender_unknown                int64\nmeans_of_attack              object\nattack_context               object\nlocation                     object\nlatitude                    float64\nlongitude                   float64\nmotive                       object\nactor_type                   object\nactor_name                   object\ndetails                      object\nverified                     object\nsource                       object\ndtype: object\n\n\n\nData Type Optimization\nThe following columns could benefit from data type conversion:\n\n\n\n\n\n\n\n\n\nColumn\nCurrent\nSuggested\nReason\n\n\n\n\nicrc\nfloat64\nint64\nCounts should be integers\n\n\nnrcs_and_ifrc\nfloat64\nint64\nCounts should be integers\n\n\nnngo\nfloat64\nint64\nCounts should be integers\n\n\n\n\n\nCode\n# Convert float columns representing counts to integers\n# Note: This will replace any NaN values with 0 during conversion\ncount_columns = ['icrc', 'nrcs_and_ifrc', 'nngo']\nfor col in count_columns:\n    if col in df.columns:\n        # Fill NaN values with 0 before converting to integer\n        df[col] = df[col].fillna(0).astype(int)\n\n# Verify the conversions\ndf[count_columns].dtypes\n\n\nicrc             int64\nnrcs_and_ifrc    int64\nnngo             int64\ndtype: object"
  },
  {
    "objectID": "data_cleaning.html#final-dataset-summary",
    "href": "data_cleaning.html#final-dataset-summary",
    "title": "Security Incidents Data Cleaning",
    "section": "8. Final Dataset Summary",
    "text": "8. Final Dataset Summary\nLet‚Äôs examine our cleaned dataset:\n\n\nCode\n# Display basic information about the cleaned dataset\nprint(f\"Final dataset shape: {df.shape}\")\nprint(\"\\nColumns in the cleaned dataset:\")\nprint(df.columns.tolist())\n\n# Display summary statistics for numeric columns\nprint(\"\\nSummary statistics:\")\ndf.describe()\n\n\nFinal dataset shape: (4314, 35)\n\nColumns in the cleaned dataset:\n['year', 'country', 'un', 'ingo', 'icrc', 'nrcs_and_ifrc', 'nngo', 'other', 'nationals_killed', 'nationals_wounded', 'nationals_kidnapped', 'total_nationals', 'internationals_killed', 'internationals_wounded', 'internationals_kidnapped', 'total_internationals', 'total_killed', 'total_wounded', 'total_kidnapped', 'total_affected', 'gender_male', 'gender_female', 'gender_unknown', 'means_of_attack', 'attack_context', 'location', 'latitude', 'longitude', 'motive', 'actor_type', 'actor_name', 'details', 'verified', 'source', 'high_impact']\n\nSummary statistics:\n\n\n\n\n\n\n\n\n\nyear\nun\ningo\nicrc\nnrcs_and_ifrc\nnngo\nother\nnationals_killed\nnationals_wounded\nnationals_kidnapped\n...\ntotal_internationals\ntotal_killed\ntotal_wounded\ntotal_kidnapped\ntotal_affected\ngender_male\ngender_female\ngender_unknown\nlatitude\nlongitude\n\n\n\n\ncount\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n...\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4301.000000\n4301.000000\n\n\nmean\n2015.245712\n0.411683\n0.812471\n0.051229\n0.116365\n0.476124\n0.024339\n0.645109\n0.631433\n0.410987\n...\n0.204682\n0.701205\n0.695874\n0.495132\n1.892211\n0.894529\n0.139314\n0.857904\n16.693709\n36.486580\n\n\nstd\n6.804200\n2.107963\n1.651766\n0.420239\n0.780231\n1.192180\n0.224018\n1.772060\n1.464470\n1.242489\n...\n0.772268\n1.818565\n1.517728\n1.366215\n2.665913\n1.217483\n0.471608\n2.531503\n14.481132\n30.046441\n\n\nmin\n1997.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n-34.883611\n-102.283333\n\n\n25%\n2010.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n5.835800\n28.729977\n\n\n50%\n2016.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n0.000000\n0.000000\n13.426301\n34.450000\n\n\n75%\n2021.000000\n0.000000\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n1.000000\n1.000000\n0.000000\n...\n0.000000\n1.000000\n1.000000\n0.000000\n2.000000\n1.000000\n0.000000\n1.000000\n32.887222\n45.400000\n\n\nmax\n2025.000000\n92.000000\n49.000000\n8.000000\n19.000000\n15.000000\n5.000000\n70.000000\n37.000000\n19.000000\n...\n15.000000\n70.000000\n37.000000\n20.000000\n92.000000\n17.000000\n7.000000\n92.000000\n52.253183\n179.012274\n\n\n\n\n8 rows √ó 24 columns"
  },
  {
    "objectID": "data_cleaning.html#conclusion",
    "href": "data_cleaning.html#conclusion",
    "title": "Security Incidents Data Cleaning",
    "section": "9. Conclusion",
    "text": "9. Conclusion\nThe data cleaning process for the security incidents dataset has:\n\nStandardized column names for consistency\nRemoved granular fields with limited analytical value\nAddressed minimal remaining missing values by leaving them as-is\nRemoved 28 duplicate records from 5 unique patterns\nOptimized data types for count-based columns\nIdentified and flagged high-impact incidents for flexible analysis\nValidated geographic coordinates, confirming all are within valid ranges\n\nThe dataset is now ready for exploratory data analysis and modeling to identify trends in aid worker security incidents by country, year, and organization type.\n\n\nCode\n# Save the cleaned dataset\ndf.to_csv(\"data/security_incidents_cleaned.csv\", index=False)"
  },
  {
    "objectID": "data_cleaning.html#handling-duplicates",
    "href": "data_cleaning.html#handling-duplicates",
    "title": "Security Incidents Data Cleaning",
    "section": "4. Handling Duplicates",
    "text": "4. Handling Duplicates\nLet‚Äôs identify and remove duplicate records that could skew our analysis.\n\n\nCode\n# Identify rows that are duplicated\nduplicated_mask = df.duplicated(keep=False)\nduplicates = df[duplicated_mask]\n\n# Count total number of duplicated rows\nduplicate_count = len(duplicates)\nprint(f\"Total number of duplicated rows: {duplicate_count}\")\n\n# Count unique duplicate patterns\nduplicate_patterns = df[duplicated_mask].groupby(df.columns.tolist()).size().reset_index()\nduplicate_patterns = duplicate_patterns.rename(columns={0: 'occurrence_count'})\n\n# Sort by occurrence count (most duplicated first)\nduplicate_patterns = duplicate_patterns.sort_values('occurrence_count', ascending=False)\n\n# Count the number of unique duplicate patterns\nunique_duplicate_patterns = len(duplicate_patterns)\nprint(f\"Number of unique duplicate patterns: {unique_duplicate_patterns}\")\n\n# Display occurrence counts (how many records appear 2 times, 3 times, etc.)\noccurrence_summary = duplicate_patterns['occurrence_count'].value_counts().sort_index()\nprint(\"\\nOccurrence pattern summary:\")\nfor count, frequency in occurrence_summary.items():\n    print(f\"  {frequency} record(s) appear {count} times each\")\n\n\nTotal number of duplicated rows: 28\nNumber of unique duplicate patterns: 5\n\nOccurrence pattern summary:\n  3 record(s) appear 2 times each\n  1 record(s) appear 9 times each\n  1 record(s) appear 13 times each\n\n\nBased on the duplicate analysis results, we need to remove duplicates from the dataset. The pattern of duplications (with some records appearing up to 13 times) suggests systematic duplication issues that could significantly skew our analysis.\n\n\nCode\n# Store original row count\noriginal_count = len(df)\n\n# Remove duplicates, keeping only the first occurrence\ndf = df.drop_duplicates()\n\n# Calculate how many rows were removed\nremoved_count = original_count - len(df)\nremoval_percentage = (removed_count / original_count) * 100\n\nprint(f\"Removed {removed_count} duplicate rows ({removal_percentage:.2f}% of dataset)\")\nprint(f\"Dataset now contains {len(df)} unique records\")\n\n\nRemoved 23 duplicate rows (0.53% of dataset)\nDataset now contains 4314 unique records"
  },
  {
    "objectID": "data_cleaning.html#data-type-optimization",
    "href": "data_cleaning.html#data-type-optimization",
    "title": "Security Incidents Data Cleaning",
    "section": "5. Data Type Optimization",
    "text": "5. Data Type Optimization\nMost data types in this dataset are appropriate, but there are a few minor adjustments that could be made for consistency:\n\n\nCode\n# Display current data types\ndf.dtypes\n\n\nyear                          int64\ncountry                      object\nun                            int64\ningo                          int64\nicrc                        float64\nnrcs_and_ifrc               float64\nnngo                        float64\nother                         int64\nnationals_killed              int64\nnationals_wounded             int64\nnationals_kidnapped           int64\ntotal_nationals               int64\ninternationals_killed         int64\ninternationals_wounded        int64\ninternationals_kidnapped      int64\ntotal_internationals          int64\ntotal_killed                  int64\ntotal_wounded                 int64\ntotal_kidnapped               int64\ntotal_affected                int64\ngender_male                   int64\ngender_female                 int64\ngender_unknown                int64\nmeans_of_attack              object\nattack_context               object\nlocation                     object\nlatitude                    float64\nlongitude                   float64\nmotive                       object\nactor_type                   object\nactor_name                   object\ndetails                      object\nverified                     object\nsource                       object\ndtype: object\n\n\n\n5.1 Data Type Conversion\nThe following columns could benefit from data type conversion:\n\n\n\n\n\n\n\n\n\nColumn\nCurrent\nSuggested\nReason\n\n\n\n\nicrc\nfloat64\nint64\nCounts should be integers\n\n\nnrcs_and_ifrc\nfloat64\nint64\nCounts should be integers\n\n\nnngo\nfloat64\nint64\nCounts should be integers\n\n\n\n\n\nCode\n# Convert float columns representing counts to integers\n# Note: This will replace any NaN values with 0 during conversion\ncount_columns = ['icrc', 'nrcs_and_ifrc', 'nngo']\nfor col in count_columns:\n    if col in df.columns:\n        # Fill NaN values with 0 before converting to integer\n        df[col] = df[col].fillna(0).astype(int)\n\n# Verify the conversions\ndf[count_columns].dtypes\n\n\nicrc             int64\nnrcs_and_ifrc    int64\nnngo             int64\ndtype: object"
  },
  {
    "objectID": "data_cleaning.html#outlier-analysis",
    "href": "data_cleaning.html#outlier-analysis",
    "title": "Security Incidents Data Cleaning",
    "section": "6. Outlier Analysis",
    "text": "6. Outlier Analysis\nLet‚Äôs identify and visualize outliers in the dataset to better understand extreme values in our security incidents data.\n\n\nCode\n# Function to detect outliers using IQR method\ndef detect_outliers(df, column):\n    q1 = df[column].quantile(0.25)\n    q3 = df[column].quantile(0.75)\n    iqr = q3 - q1\n    lower_bound = q1 - 1.5 * iqr\n    upper_bound = q3 + 1.5 * iqr\n    \n    outliers = df[(df[column] &lt; lower_bound) | (df[column] &gt; upper_bound)]\n    return outliers, lower_bound, upper_bound\n\n# Get list of numeric columns (excluding some that don't need outlier analysis)\nexcluded_cols = ['year']\nnumeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\nnumeric_cols = [col for col in numeric_cols if col not in excluded_cols]\n\n# Create a DataFrame to store outlier summary\noutlier_summary = pd.DataFrame(columns=['Column', 'Total', 'Outliers', 'Percentage'])\n\n# Collect outlier information\nfor col in numeric_cols:\n    outliers, _, _ = detect_outliers(df, col)\n    \n    # Add to summary DataFrame\n    new_row = {\n        'Column': col,\n        'Total': len(df),\n        'Outliers': len(outliers),\n        'Percentage': len(outliers) / len(df) * 100\n    }\n    outlier_summary = pd.concat([outlier_summary, pd.DataFrame([new_row])], ignore_index=True)\n\n# Sort by percentage of outliers (descending)\noutlier_summary = outlier_summary.sort_values('Percentage', ascending=False)\n\n# Visualize outlier percentages\nplt.figure(figsize=(12, 6))\nsns.barplot(x='Percentage', y='Column', data=outlier_summary, palette='viridis')\nplt.title('Percentage of Outliers by Column')\nplt.xlabel('Percentage of Values Identified as Outliers')\nplt.grid(axis='x', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n/var/folders/gf/ppk7yck96gx15bzg93b7fysm0000gn/T/ipykernel_16401/3183464735.py:31: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/var/folders/gf/ppk7yck96gx15bzg93b7fysm0000gn/T/ipykernel_16401/3183464735.py:38: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n\n6.1 Visualization of Key Outliers\nLet‚Äôs visualize the distribution and outliers for the most important columns:\n\n\nCode\n# Select top 6 columns with the most outliers for detailed visualization\ntop_cols = outlier_summary.head(6)['Column'].tolist()\n\n# Create boxplots for top outlier columns\nplt.figure(figsize=(15, 10))\nfor i, col in enumerate(top_cols, 1):\n    plt.subplot(2, 3, i)\n    sns.boxplot(y=df[col])\n    plt.title(f'Box Plot: {col}')\n    plt.grid(linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n6.2 Outlier Handling Strategy\nIn security incident data, outliers often represent real extreme events (like major attacks) rather than errors. For our analysis:\n\nWe‚Äôll flag high-impact incidents rather than removing them\nThis allows us to analyze with or without extreme events as needed\n\n\n\nCode\n# Create flags for high-impact incidents\ndf['high_impact'] = False\n\n# Flag incidents with casualties in the top 1% of any category\nfor col in ['total_killed', 'total_wounded', 'total_kidnapped', 'total_affected']:\n    if col in df.columns:\n        threshold = df[col].quantile(0.99)\n        df.loc[df[col] &gt; threshold, 'high_impact'] = True\n\n# Print summary of flagged high-impact incidents\nhigh_impact_count = df['high_impact'].sum()\nprint(f\"Flagged {high_impact_count} high-impact incidents ({high_impact_count/len(df)*100:.2f}% of dataset)\")\n\n# Example: Look at the top 5 most severe incidents by total casualties\nif 'total_affected' in df.columns:\n    print(\"\\nTop 5 most severe incidents:\")\n    display(df.sort_values('total_affected', ascending=False).head(5)[\n        ['year', 'country', 'total_affected', 'total_killed', 'total_wounded', 'total_kidnapped']])\n\n\nFlagged 123 high-impact incidents (2.85% of dataset)\n\nTop 5 most severe incidents:\n\n\n\n\n\n\n\n\n\nyear\ncountry\ntotal_affected\ntotal_killed\ntotal_wounded\ntotal_kidnapped\n\n\n\n\n3857\n2023\nOccupied Palestinian Territories\n92\n70\n22\n0\n\n\n1989\n2015\nAfghanistan\n49\n14\n35\n0\n\n\n3920\n2023\nOccupied Palestinian Territories\n46\n41\n5\n0\n\n\n1189\n2011\nNigeria\n46\n9\n37\n0\n\n\n3936\n2023\nOccupied Palestinian Territories\n31\n31\n0\n0"
  },
  {
    "objectID": "data_cleaning.html#outlier-analysis-and-handling",
    "href": "data_cleaning.html#outlier-analysis-and-handling",
    "title": "Security Incidents Data Cleaning",
    "section": "6. Outlier Analysis and Handling",
    "text": "6. Outlier Analysis and Handling\nLet‚Äôs identify and visualize outliers in the dataset to better understand extreme values in our security incidents data.\n\n\nCode\n# Function to detect outliers using IQR method\ndef detect_outliers(df, column):\n    q1 = df[column].quantile(0.25)\n    q3 = df[column].quantile(0.75)\n    iqr = q3 - q1\n    lower_bound = q1 - 1.5 * iqr\n    upper_bound = q3 + 1.5 * iqr\n    \n    outliers = df[(df[column] &lt; lower_bound) | (df[column] &gt; upper_bound)]\n    return outliers, lower_bound, upper_bound\n\n# Get list of numeric columns (excluding some that don't need outlier analysis)\nexcluded_cols = ['year']\nnumeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\nnumeric_cols = [col for col in numeric_cols if col not in excluded_cols]\n\n# Create a DataFrame to store outlier summary\noutlier_summary = pd.DataFrame(columns=['Column', 'Total', 'Outliers', 'Percentage'])\n\n# Collect outlier information\nfor col in numeric_cols:\n    outliers, _, _ = detect_outliers(df, col)\n    \n    # Add to summary DataFrame\n    new_row = {\n        'Column': col,\n        'Total': len(df),\n        'Outliers': len(outliers),\n        'Percentage': len(outliers) / len(df) * 100\n    }\n    outlier_summary = pd.concat([outlier_summary, pd.DataFrame([new_row])], ignore_index=True)\n\n# Sort by percentage of outliers (descending)\noutlier_summary = outlier_summary.sort_values('Percentage', ascending=False)\n\n# Visualize outlier percentages\nplt.figure(figsize=(7,4))\nsns.barplot(x='Percentage', y='Column', data=outlier_summary, palette='viridis')\nplt.title('Percentage of Outliers by Column')\nplt.xlabel('Percentage of Values Identified as Outliers')\nplt.grid(axis='x', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n/var/folders/gf/ppk7yck96gx15bzg93b7fysm0000gn/T/ipykernel_18743/875423659.py:31: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/var/folders/gf/ppk7yck96gx15bzg93b7fysm0000gn/T/ipykernel_18743/875423659.py:38: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n\n6.1 Visualization of Key Outliers\nLet‚Äôs visualize the distribution and outliers for the most important columns:\n\n\nCode\n# Select top 6 columns with the most outliers for detailed visualization\ntop_cols = outlier_summary.head(6)['Column'].tolist()\n\n# Create boxplots for top outlier columns\nplt.figure(figsize=(10, 6))\nfor i, col in enumerate(top_cols, 1):\n    plt.subplot(2, 3, i)\n    sns.boxplot(y=df[col])\n    plt.title(f'Box Plot: {col}')\n    plt.grid(linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n6.2 Outlier Handling Strategy\nIn security incident data, outliers often represent real extreme events (like major attacks) rather than errors. For our analysis:\n\nWe‚Äôll flag high-impact incidents rather than removing them\nThis allows us to analyze with or without extreme events as needed\n\n\n\nCode\n# Create flags for high-impact incidents\ndf['high_impact'] = False\n\n# Flag incidents with casualties in the top 1% of any category\nfor col in ['total_killed', 'total_wounded', 'total_kidnapped', 'total_affected']:\n    if col in df.columns:\n        threshold = df[col].quantile(0.99)\n        df.loc[df[col] &gt; threshold, 'high_impact'] = True\n\n# Print summary of flagged high-impact incidents\nhigh_impact_count = df['high_impact'].sum()\nprint(f\"Flagged {high_impact_count} high-impact incidents ({high_impact_count/len(df)*100:.2f}% of dataset)\")\n\n# Example: Look at the top 5 most severe incidents by total casualties\nif 'total_affected' in df.columns:\n    print(\"\\nTop 5 most severe incidents:\")\n    display(df.sort_values('total_affected', ascending=False).head(5)[\n        ['year', 'country', 'total_affected', 'total_killed', 'total_wounded', 'total_kidnapped']])\n\n\nFlagged 123 high-impact incidents (2.85% of dataset)\n\nTop 5 most severe incidents:\n\n\n\n\n\n\n\n\n\nyear\ncountry\ntotal_affected\ntotal_killed\ntotal_wounded\ntotal_kidnapped\n\n\n\n\n3857\n2023\nOccupied Palestinian Territories\n92\n70\n22\n0\n\n\n1989\n2015\nAfghanistan\n49\n14\n35\n0\n\n\n3920\n2023\nOccupied Palestinian Territories\n46\n41\n5\n0\n\n\n1189\n2011\nNigeria\n46\n9\n37\n0\n\n\n3936\n2023\nOccupied Palestinian Territories\n31\n31\n0\n0"
  },
  {
    "objectID": "data_cleaning.html#geographic-data-validation",
    "href": "data_cleaning.html#geographic-data-validation",
    "title": "Security Incidents Data Cleaning",
    "section": "7. Geographic Data Validation",
    "text": "7. Geographic Data Validation\nLet‚Äôs verify that our latitude and longitude values are within valid ranges.\n\n\nCode\n# Check if latitude and longitude values are within valid ranges\n# Valid ranges: Latitude (-90 to 90), Longitude (-180 to 180)\n\n# Count invalid coordinates\ninvalid_lat = df[(df['latitude'] &lt; -90) | (df['latitude'] &gt; 90)].shape[0]\ninvalid_lon = df[(df['longitude'] &lt; -180) | (df['longitude'] &gt; 180)].shape[0]\n\nprint(f\"Invalid latitude values (outside -90 to 90): {invalid_lat}\")\nprint(f\"Invalid longitude values (outside -180 to 180): {invalid_lon}\")\n\n# Get overall ranges to see extreme values\nlat_min, lat_max = df['latitude'].min(), df['latitude'].max()\nlon_min, lon_max = df['longitude'].min(), df['longitude'].max()\n\nprint(f\"\\nLatitude range: {lat_min} to {lat_max}\")\nprint(f\"Longitude range: {lon_min} to {lon_max}\")\n\n# Create a scatter plot to visualize coordinate distribution\nplt.figure(figsize=(7,4))\nplt.scatter(df['longitude'], df['latitude'], alpha=0.5, s=3)\nplt.title('Geographic Distribution of Incidents')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.xlim(-180, 180)\nplt.ylim(-90, 90)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\nInvalid latitude values (outside -90 to 90): 0\nInvalid longitude values (outside -180 to 180): 0\n\nLatitude range: -34.883611 to 52.253183\nLongitude range: -102.283333 to 179.0122737\n\n\n\n\n\n\n\n\n\nThere are 0 invalid latitudes and longitudes, so we do not have to handle those outliers. All geographic coordinates are within valid ranges and suitable for mapping and spatial analysis."
  }
]