[
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Security Incidents Data Cleaning",
    "section": "",
    "text": "This document outlines the data cleaning process for the security incidents dataset. We‚Äôll examine the data structure, identify and address missing values, and prepare the dataset for analysis of broad trends in aid worker incidents by country, year, and organization type.\nThe cleaning process follows a systematic approach:\n\nLoading and initial inspection\nHandling missing values\nDuplicate detection and removal\nData type optimization\nOutlier analysis and handling\nGeographic data validation\nFinal dataset preparation"
  },
  {
    "objectID": "data_cleaning.html#step-1-exploring-what-needs-cleaning",
    "href": "data_cleaning.html#step-1-exploring-what-needs-cleaning",
    "title": "Data Cleaning",
    "section": "Step 1: Exploring What Needs Cleaning",
    "text": "Step 1: Exploring What Needs Cleaning\nBefore cleaning the dataset, let‚Äôs inspect the structure, missing values, and data types to understand what issues we need to address.\n\n\nCode\ndf.shape\n\n\n(4337, 41)\n\n\nThere are 41 columns and 4337 rows.\n\n\nCode\ndf.dtypes\n\n\nincident_id                   int64\nyear                          int64\nmonth                       float64\nday                         float64\ncountry_code                 object\ncountry                      object\nregion                       object\ndistrict                     object\ncity                         object\nun                            int64\ningo                          int64\nicrc                        float64\nnrcs_and_ifrc               float64\nnngo                        float64\nother                         int64\nnationals_killed              int64\nnationals_wounded             int64\nnationals_kidnapped           int64\ntotal_nationals               int64\ninternationals_killed         int64\ninternationals_wounded        int64\ninternationals_kidnapped      int64\ntotal_internationals          int64\ntotal_killed                  int64\ntotal_wounded                 int64\ntotal_kidnapped               int64\ntotal_affected                int64\ngender_male                   int64\ngender_female                 int64\ngender_unknown                int64\nmeans_of_attack              object\nattack_context               object\nlocation                     object\nlatitude                    float64\nlongitude                   float64\nmotive                       object\nactor_type                   object\nactor_name                   object\ndetails                      object\nverified                     object\nsource                       object\ndtype: object\n\n\nMost data types in this dataset are appropriate. Columns like incident_id and year are correctly stored as integers, while text-based fields such as country, region, motive, and actor_type are stored as objects, which is suitable for categorical or descriptive information. Count-based columns like un, ingo, other, total_, and gender_ are also properly stored as integers. However, month and day are currently floats due to missing values, but should be converted to integers after filling. Similarly, icrc, nrcs_and_ifrc, and nngo are counts but stored as floats, so converting them to integers will improve consistency. Latitude and longitude are correctly stored as floats since they contain decimal values. Overall, the data types are mostly correct, with just a few minor adjustments needed.\n\nüõ† Optional Data Type Fixes\n\n\n\n\n\n\n\n\n\nColumn\nCurrent\nSuggested\nReason\n\n\n\n\nmonth, day\nfloat64\nint64\nWhole numbers; convert after filling missing values.\n\n\nicrc\nfloat64\nint64\nCount; cleaner as integer.\n\n\nnrcs_and_ifrc\nfloat64\nint64\nCount; cleaner as integer.\n\n\nnngo\nfloat64\nint64\nCount; cleaner as integer.\n\n\n\n\n\nCode\n# Calculate percent of missing values per column\nmissing_percent = (df.isna().sum() / len(df)) * 100\nmissing_percent = missing_percent[missing_percent &gt; 0].sort_values(ascending=True)\n\n# Re-plot with smaller dimensions\nplt.figure(figsize=(7, 4))  # reduced size\nmissing_percent.plot(kind='barh')\nplt.title(\"Percentage of Missing Values per Column\")\nplt.xlabel(\"Percent Missing (%)\")\nplt.tight_layout()\nplt.grid(axis='x', linestyle='--', alpha=0.5)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nüîç Step 2: Visualizing Missing Data\nTo better understand which variables have missing values, we created a horizontal bar plot showing the percentage of missing values per column. This makes it easier to identify which variables might require imputation, removal, or further attention.\nAs seen below, City, District, Day, and Region have the highest proportions of missing data, with City missing in over 20% of records.\n\n\nüóë Step 3: Dropping Low-Value or Redundant Columns\nWhile some columns like Day, City, District, and Region provide detailed location and date information, they have a high proportion of missing values and offer limited analytical value for our high-level analysis.\nSince our goal is to examine broad trends in aid worker incidents by country, year, and organization type, we decided to remove these granular fields:\n\nDay and Month: Year is sufficient for time trends.\nDistrict, City, Region: Country-level insights are the focus."
  },
  {
    "objectID": "index.html#about-this-project",
    "href": "index.html#about-this-project",
    "title": "Humanitarian Aid Under Fire",
    "section": "About This Project",
    "text": "About This Project"
  },
  {
    "objectID": "index.html#about-this-dataset-aid-worker-security-incidents",
    "href": "index.html#about-this-dataset-aid-worker-security-incidents",
    "title": "Humanitarian Aid Under Fire",
    "section": "üîê About This Dataset: Aid Worker Security Incidents",
    "text": "üîê About This Dataset: Aid Worker Security Incidents\nThis dataset security_incidents.csv comes from the Aid Worker Security Database (AWSD), a global resource that tracks major incidents of violence against humanitarian aid workers. Maintained by Humanitarian Outcomes, the AWSD is a leading source used by researchers, NGOs, and policy makers to understand risks faced by aid personnel in the field."
  },
  {
    "objectID": "index.html#whats-included-in-the-data",
    "href": "index.html#whats-included-in-the-data",
    "title": "Humanitarian Aid Under Fire",
    "section": "üìä What‚Äôs Included in the Data?",
    "text": "üìä What‚Äôs Included in the Data?\nEach row in this dataset represents a documented security incident involving humanitarian organizations. Key information includes:\n\nDate and Location: Year, Month, Country, Region, District, City, Coordinates\n\nOrganization Type: UN, INGOs, NNGOs, ICRC/IFRC, Other aid actors\n\nIncident Impact: Number of nationals and internationals killed, wounded, or kidnapped\n\nAttack Details: Type of attack (e.g., shooting, kidnapping), context (e.g., ambush, raid), and actor information\n\nAdditional Fields: Motive, actor type and name, description, and data verification status"
  },
  {
    "objectID": "index.html#key-questions",
    "href": "index.html#key-questions",
    "title": "Humanitarian Aid Under Fire",
    "section": "Key Questions",
    "text": "Key Questions\n\nmermaid diagram"
  },
  {
    "objectID": "index.html#why-this-matters",
    "href": "index.html#why-this-matters",
    "title": "Humanitarian Aid Under Fire",
    "section": "üåç Why This Matters",
    "text": "üåç Why This Matters\nThe AWSD is the only comprehensive global database that documents targeted violence against aid workers. It distinguishes between national and international staff, allowing deeper insights into the differential risks they face. The database supports humanitarian access planning, risk assessment, and evidence-based security policy.\n\nüìÇ Explore More\n\nVisualizations\nMethods\nData Summary\nDownload the Dataset"
  },
  {
    "objectID": "index.html#source-and-access",
    "href": "index.html#source-and-access",
    "title": "Humanitarian Aid Under Fire",
    "section": "üì• Source and Access",
    "text": "üì• Source and Access\nThis dataset was downloaded directly from: üëâ Aid Worker Security Database (AWSD)\nPlease cite as:\nHumanitarian Outcomes. Aid Worker Security Database (AWSD). https://aidworkersecurity.org"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "data_cleaning.html#missing-values",
    "href": "data_cleaning.html#missing-values",
    "title": "Data Cleaning",
    "section": "Missing Values",
    "text": "Missing Values\nTo better understand which variables have missing values, we created a horizontal bar plot showing the percentage of missing values per column. This makes it easier to identify which variables might require imputation, removal, or further attention.\nAs seen below, City, District, Day, and Region have the highest proportions of missing data, with City missing in over 20% of records.\n\n\nCode\n# Calculate percent of missing values per column\nmissing_percent = (df.isna().sum() / len(df)) * 100\nmissing_percent = missing_percent[missing_percent &gt; 0].sort_values(ascending=True)\n\n# Re-plot with smaller dimensions\nplt.figure(figsize=(7, 4))  # reduced size\nmissing_percent.plot(kind='barh')\nplt.title(\"Percentage of Missing Values per Column\")\nplt.xlabel(\"Percent Missing (%)\")\nplt.tight_layout()\nplt.grid(axis='x', linestyle='--', alpha=0.5)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nWhile some columns like Day, Month, City, District, and Region provide detailed location and date information, they have a high proportion of missing values and offer limited analytical value for our analysis.\nSince our goal is to examine broad trends in aid worker incidents by country, year, and organization type, we decided to remove these granular fields:\n\nDay and Month: Year is sufficient for time trends.\nDistrict, City, Region, Country Code: Country-level insights are the focus.\n\n\n\nCode\ncolumns_to_drop = ['day', 'month', 'district', 'city', 'region', 'country_code']\ndf.drop(columns=columns_to_drop, inplace=True)\n\n\n\n\nCode\n# Calculate percent of missing values per column\nmissing_percent = (df.isna().sum() / len(df)) * 100\nmissing_percent = missing_percent[missing_percent &gt; 0].sort_values(ascending=True)\n\n# Re-plot with smaller dimensions\nplt.figure(figsize=(7, 4))  # reduced size\nmissing_percent.plot(kind='barh')\nplt.title(\"Percentage of Missing Values per Column\")\nplt.xlabel(\"Percent Missing (%)\")\nplt.tight_layout()\nplt.grid(axis='x', linestyle='--', alpha=0.5)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nSince the highest missing value is less than 0.5%, and the remaining missing data is sparse, likely random, and not concentrated in critical columns, no imputation is necessary. We chose to leave these values as-is (NaN), as most analysis tools handle them gracefully and imputing could introduce unnecessary bias."
  },
  {
    "objectID": "data_cleaning.html#exploring-what-needs-cleaning",
    "href": "data_cleaning.html#exploring-what-needs-cleaning",
    "title": "Data Cleaning",
    "section": "Exploring What Needs Cleaning",
    "text": "Exploring What Needs Cleaning\nBefore cleaning the dataset, let‚Äôs inspect the structure, missing values, and data types to understand what issues we need to address.\n\n\nCode\nrows, cols = df.shape\nprint(f\"The dataset contains {rows} rows and {cols} columns.\")\n\n\nThe dataset contains 4337 rows and 35 columns.\n\n\nNext, we explore data types. Most data types in this dataset are appropriate. Columns like incident_id and year are correctly stored as integers, while text-based fields such as country, region, motive, and actor_type are stored as objects, which is suitable for categorical or descriptive information. Count-based columns like un, ingo, other, total_, and gender_ are also properly stored as integers. However, month and day are currently floats due to missing values, but should be converted to integers after filling. Similarly, icrc, nrcs_and_ifrc, and nngo are counts but stored as floats, so converting them to integers will improve consistency. Latitude and longitude are correctly stored as floats since they contain decimal values. Overall, the data types are mostly correct, with just a few minor adjustments needed.\n\nüõ† Optional Data Type Fixes\n\n\n\n\n\n\n\n\n\nColumn\nCurrent\nSuggested\nReason\n\n\n\n\nmonth, day\nfloat64\nint64\nWhole numbers; convert after filling missing values.\n\n\nicrc\nfloat64\nint64\nCount; cleaner as integer.\n\n\nnrcs_and_ifrc\nfloat64\nint64\nCount; cleaner as integer.\n\n\nnngo\nfloat64\nint64\nCount; cleaner as integer."
  },
  {
    "objectID": "data_cleaning.html#introduction",
    "href": "data_cleaning.html#introduction",
    "title": "Security Incidents Data Cleaning",
    "section": "",
    "text": "This document outlines the data cleaning process for the security incidents dataset. We‚Äôll examine the data structure, identify and address missing values, and prepare the dataset for analysis of broad trends in aid worker incidents by country, year, and organization type.\nThe cleaning process follows a systematic approach:\n\nLoading and initial inspection\nHandling missing values\nDuplicate detection and removal\nData type optimization\nOutlier analysis and handling\nGeographic data validation\nFinal dataset preparation"
  },
  {
    "objectID": "data_cleaning.html#loading-and-initial-inspection",
    "href": "data_cleaning.html#loading-and-initial-inspection",
    "title": "Security Incidents Data Cleaning",
    "section": "2. Loading and Initial Inspection",
    "text": "2. Loading and Initial Inspection\nFirst, let‚Äôs load the dataset and standardize the column names for consistency.\n\n\nCode\n# Import required libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Load the security incidents dataset\ndf = pd.read_csv(\"data/security_incidents.csv\")\n\n# Standardize column names (lowercase, replace spaces with underscores)\ndf.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n\n# Display the first few rows\ndf.head(1)\n\n\n\n\n\n\n\n\n\nincident_id\nyear\nmonth\nday\ncountry_code\ncountry\nregion\ndistrict\ncity\nun\n...\nattack_context\nlocation\nlatitude\nlongitude\nmotive\nactor_type\nactor_name\ndetails\nverified\nsource\n\n\n\n\n0\n1\n1997\n1.0\nNaN\nKH\nCambodia\nBanteay Meanchey\nNaN\nNaN\n0\n...\nUnknown\nUnknown\n14.070929\n103.099916\nUnknown\nUnknown\nUnknown\n1 ICRC national staff killed while working in ...\nArchived\nArchived\n\n\n\n\n1 rows √ó 41 columns\n\n\n\n\n\nCode\n# Get basic dataset information\nrows, cols = df.shape\nprint(f\"The dataset contains {rows} rows and {cols} columns.\")\n\n\nThe dataset contains 4337 rows and 41 columns."
  },
  {
    "objectID": "data_cleaning.html#handling-missing-values",
    "href": "data_cleaning.html#handling-missing-values",
    "title": "Security Incidents Data Cleaning",
    "section": "3. Handling Missing Values",
    "text": "3. Handling Missing Values\nTo better understand which variables have missing values, we‚Äôll create a visualization showing the percentage of missing values per column.\n\n\nCode\n# Calculate percent of missing values per column\nmissing_percent = (df.isna().sum() / len(df)) * 100\nmissing_percent = missing_percent[missing_percent &gt; 0].sort_values(ascending=True)\n\n# Create a horizontal bar plot of missing values\nplt.figure(figsize=(7, 4))\nmissing_percent.plot(kind='barh')\nplt.title(\"Percentage of Missing Values per Column\")\nplt.xlabel(\"Percent Missing (%)\")\nplt.tight_layout()\nplt.grid(axis='x', linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Check for missing values\nmissing_values = df.isnull().sum()\nmissing_percent = (missing_values / len(df) * 100).round(2)\n\n# Create a dataframe of missing values\nmissing_df = pd.DataFrame({\n    'Missing Values': missing_values,\n    'Percentage': missing_percent\n})\n\n# Display columns with missing values\nprint(\"Columns with missing values:\")\nmissing_df[missing_df['Missing Values'] &gt; 0].sort_values('Missing Values', ascending=False)\n\n# Visualize missing values if any exist\nif missing_values.sum() &gt; 0:\n    plt.figure(figsize=(12, 6))\n    sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n    plt.title('Missing Values Heatmap')\n    plt.tight_layout()\n    plt.show()\n\n\nColumns with missing values:\n\n\n\n\n\n\n\n\n\n\n3.1 Strategy for Missing Values\nBased on our exploration, we have identified several columns with high proportions of missing values:\n\nCity (over 20% missing)\nDistrict, Day, and Region (significant proportions missing)\n\nSince our analysis focus is on broad trends by country, year, and organization type, we will:\n\nRemove granular fields with limited analytical value for our specific goals\nLeave the remaining minimal missing values as-is, as they‚Äôre sparse and likely random\n\n\n\nCode\n# Remove columns with limited analytical value for our specific analysis goals\ncolumns_to_drop = ['day', 'month', 'district', 'city', 'region', 'country_code', 'incident_id']\ndf.drop(columns=columns_to_drop, inplace=True)\n\n# Recalculate missing values after dropping columns\nmissing_percent = (df.isna().sum() / len(df)) * 100\nmissing_percent = missing_percent[missing_percent &gt; 0].sort_values(ascending=True)\n\n# Visualize the remaining missing values\nplt.figure(figsize=(7, 4))\nmissing_percent.plot(kind='barh')\nplt.title(\"Percentage of Missing Values After Column Removal\")\nplt.xlabel(\"Percent Missing (%)\")\nplt.tight_layout()\nplt.grid(axis='x', linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Check for missing values\nmissing_values = df.isnull().sum()\nmissing_percent = (missing_values / len(df) * 100).round(2)\n\n# Create a dataframe of missing values\nmissing_df = pd.DataFrame({\n    'Missing Values': missing_values,\n    'Percentage': missing_percent\n})\n\n# Display columns with missing values\nprint(\"Columns with missing values:\")\nmissing_df[missing_df['Missing Values'] &gt; 0].sort_values('Missing Values', ascending=False)\n\n# Visualize missing values if any exist\nif missing_values.sum() &gt; 0:\n    plt.figure(figsize=(12, 6))\n    sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n    plt.title('Missing Values Heatmap')\n    plt.tight_layout()\n    plt.show()\n\n\nColumns with missing values:\n\n\n\n\n\n\n\n\n\n\n\n3.2 Missing Values Decision\nSince the highest missing value percentage is now less than 0.5%, and the remaining missing data is sparse, likely random, and not concentrated in critical columns, we‚Äôve decided:\n\nNo imputation is necessary\nLeave these values as-is (NaN), as most analysis tools handle them gracefully\nImputing could introduce unnecessary bias given the small percentage"
  },
  {
    "objectID": "data_cleaning.html#data-type-considerations",
    "href": "data_cleaning.html#data-type-considerations",
    "title": "Security Incidents Data Cleaning",
    "section": "Data Type Considerations",
    "text": "Data Type Considerations\nMost data types in this dataset are appropriate, but there are a few minor adjustments that could be made for consistency:\n\n\nCode\n# Display current data types\ndf.dtypes\n\n\nincident_id                   int64\nyear                          int64\ncountry                      object\nun                            int64\ningo                          int64\nicrc                        float64\nnrcs_and_ifrc               float64\nnngo                        float64\nother                         int64\nnationals_killed              int64\nnationals_wounded             int64\nnationals_kidnapped           int64\ntotal_nationals               int64\ninternationals_killed         int64\ninternationals_wounded        int64\ninternationals_kidnapped      int64\ntotal_internationals          int64\ntotal_killed                  int64\ntotal_wounded                 int64\ntotal_kidnapped               int64\ntotal_affected                int64\ngender_male                   int64\ngender_female                 int64\ngender_unknown                int64\nmeans_of_attack              object\nattack_context               object\nlocation                     object\nlatitude                    float64\nlongitude                   float64\nmotive                       object\nactor_type                   object\nactor_name                   object\ndetails                      object\nverified                     object\nsource                       object\ndtype: object\n\n\n\nData Type Optimization\nThe following columns could benefit from data type conversion:\n\n\n\n\n\n\n\n\n\nColumn\nCurrent\nSuggested\nReason\n\n\n\n\nicrc\nfloat64\nint64\nCounts should be integers\n\n\nnrcs_and_ifrc\nfloat64\nint64\nCounts should be integers\n\n\nnngo\nfloat64\nint64\nCounts should be integers\n\n\n\n\n\nCode\n# Convert float columns representing counts to integers\n# Note: This will replace any NaN values with 0 during conversion\ncount_columns = ['icrc', 'nrcs_and_ifrc', 'nngo']\nfor col in count_columns:\n    if col in df.columns:\n        # Fill NaN values with 0 before converting to integer\n        df[col] = df[col].fillna(0).astype(int)\n\n# Verify the conversions\ndf[count_columns].dtypes\n\n\nicrc             int64\nnrcs_and_ifrc    int64\nnngo             int64\ndtype: object"
  },
  {
    "objectID": "data_cleaning.html#final-dataset-summary",
    "href": "data_cleaning.html#final-dataset-summary",
    "title": "Security Incidents Data Cleaning",
    "section": "8. Final Dataset Summary",
    "text": "8. Final Dataset Summary\nLet‚Äôs examine our cleaned dataset:\n\n\nCode\n# Display basic information about the cleaned dataset\nprint(f\"Final dataset shape: {df.shape}\")\nprint(\"\\nColumns in the cleaned dataset:\")\nprint(df.columns.tolist())\n\n# Display summary statistics for numeric columns\nprint(\"\\nSummary statistics:\")\ndf.describe()\n\n\nFinal dataset shape: (4314, 35)\n\nColumns in the cleaned dataset:\n['year', 'country', 'un', 'ingo', 'icrc', 'nrcs_and_ifrc', 'nngo', 'other', 'nationals_killed', 'nationals_wounded', 'nationals_kidnapped', 'total_nationals', 'internationals_killed', 'internationals_wounded', 'internationals_kidnapped', 'total_internationals', 'total_killed', 'total_wounded', 'total_kidnapped', 'total_affected', 'gender_male', 'gender_female', 'gender_unknown', 'means_of_attack', 'attack_context', 'location', 'latitude', 'longitude', 'motive', 'actor_type', 'actor_name', 'details', 'verified', 'source', 'high_impact']\n\nSummary statistics:\n\n\n\n\n\n\n\n\n\nyear\nun\ningo\nicrc\nnrcs_and_ifrc\nnngo\nother\nnationals_killed\nnationals_wounded\nnationals_kidnapped\n...\ntotal_internationals\ntotal_killed\ntotal_wounded\ntotal_kidnapped\ntotal_affected\ngender_male\ngender_female\ngender_unknown\nlatitude\nlongitude\n\n\n\n\ncount\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n...\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4314.000000\n4301.000000\n4301.000000\n\n\nmean\n2015.245712\n0.411683\n0.812471\n0.051229\n0.116365\n0.476124\n0.024339\n0.645109\n0.631433\n0.410987\n...\n0.204682\n0.701205\n0.695874\n0.495132\n1.892211\n0.894529\n0.139314\n0.857904\n16.693709\n36.486580\n\n\nstd\n6.804200\n2.107963\n1.651766\n0.420239\n0.780231\n1.192180\n0.224018\n1.772060\n1.464470\n1.242489\n...\n0.772268\n1.818565\n1.517728\n1.366215\n2.665913\n1.217483\n0.471608\n2.531503\n14.481132\n30.046441\n\n\nmin\n1997.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n-34.883611\n-102.283333\n\n\n25%\n2010.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n5.835800\n28.729977\n\n\n50%\n2016.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n0.000000\n0.000000\n13.426301\n34.450000\n\n\n75%\n2021.000000\n0.000000\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n1.000000\n1.000000\n0.000000\n...\n0.000000\n1.000000\n1.000000\n0.000000\n2.000000\n1.000000\n0.000000\n1.000000\n32.887222\n45.400000\n\n\nmax\n2025.000000\n92.000000\n49.000000\n8.000000\n19.000000\n15.000000\n5.000000\n70.000000\n37.000000\n19.000000\n...\n15.000000\n70.000000\n37.000000\n20.000000\n92.000000\n17.000000\n7.000000\n92.000000\n52.253183\n179.012274\n\n\n\n\n8 rows √ó 24 columns"
  },
  {
    "objectID": "data_cleaning.html#conclusion",
    "href": "data_cleaning.html#conclusion",
    "title": "Security Incidents Data Cleaning",
    "section": "9. Conclusion",
    "text": "9. Conclusion\nThe data cleaning process for the security incidents dataset has:\n\nStandardized column names for consistency\nRemoved granular fields with limited analytical value\nAddressed minimal remaining missing values by leaving them as-is\nRemoved 28 duplicate records from 5 unique patterns\nOptimized data types for count-based columns\nIdentified and flagged high-impact incidents for flexible analysis\nValidated geographic coordinates, confirming all are within valid ranges\n\nThe dataset is now ready for exploratory data analysis and modeling to identify trends in aid worker security incidents by country, year, and organization type.\n\n\nCode\n# Save the cleaned dataset\ndf.to_csv(\"data/security_incidents_cleaned.csv\", index=False)"
  },
  {
    "objectID": "data_cleaning.html#handling-duplicates",
    "href": "data_cleaning.html#handling-duplicates",
    "title": "Security Incidents Data Cleaning",
    "section": "4. Handling Duplicates",
    "text": "4. Handling Duplicates\nLet‚Äôs identify and remove duplicate records that could skew our analysis.\n\n\nCode\n# Identify rows that are duplicated\nduplicated_mask = df.duplicated(keep=False)\nduplicates = df[duplicated_mask]\n\n# Count total number of duplicated rows\nduplicate_count = len(duplicates)\nprint(f\"Total number of duplicated rows: {duplicate_count}\")\n\n# Count unique duplicate patterns\nduplicate_patterns = df[duplicated_mask].groupby(df.columns.tolist()).size().reset_index()\nduplicate_patterns = duplicate_patterns.rename(columns={0: 'occurrence_count'})\n\n# Sort by occurrence count (most duplicated first)\nduplicate_patterns = duplicate_patterns.sort_values('occurrence_count', ascending=False)\n\n# Count the number of unique duplicate patterns\nunique_duplicate_patterns = len(duplicate_patterns)\nprint(f\"Number of unique duplicate patterns: {unique_duplicate_patterns}\")\n\n# Display occurrence counts (how many records appear 2 times, 3 times, etc.)\noccurrence_summary = duplicate_patterns['occurrence_count'].value_counts().sort_index()\nprint(\"\\nOccurrence pattern summary:\")\nfor count, frequency in occurrence_summary.items():\n    print(f\"  {frequency} record(s) appear {count} times each\")\n\n\nTotal number of duplicated rows: 28\nNumber of unique duplicate patterns: 5\n\nOccurrence pattern summary:\n  3 record(s) appear 2 times each\n  1 record(s) appear 9 times each\n  1 record(s) appear 13 times each\n\n\nBased on the duplicate analysis results, we need to remove duplicates from the dataset. The pattern of duplications (with some records appearing up to 13 times) suggests systematic duplication issues that could significantly skew our analysis.\n\n\nCode\n# Store original row count\noriginal_count = len(df)\n\n# Remove duplicates, keeping only the first occurrence\ndf = df.drop_duplicates()\n\n# Calculate how many rows were removed\nremoved_count = original_count - len(df)\nremoval_percentage = (removed_count / original_count) * 100\n\nprint(f\"Removed {removed_count} duplicate rows ({removal_percentage:.2f}% of dataset)\")\nprint(f\"Dataset now contains {len(df)} unique records\")\n\n\nRemoved 23 duplicate rows (0.53% of dataset)\nDataset now contains 4314 unique records"
  },
  {
    "objectID": "data_cleaning.html#data-type-optimization",
    "href": "data_cleaning.html#data-type-optimization",
    "title": "Security Incidents Data Cleaning",
    "section": "5. Data Type Optimization",
    "text": "5. Data Type Optimization\nMost data types in this dataset are appropriate, but there are a few minor adjustments that could be made for consistency:\n\n\nCode\n# Display current data types\ndf.dtypes\n\n\nyear                          int64\ncountry                      object\nun                            int64\ningo                          int64\nicrc                        float64\nnrcs_and_ifrc               float64\nnngo                        float64\nother                         int64\nnationals_killed              int64\nnationals_wounded             int64\nnationals_kidnapped           int64\ntotal_nationals               int64\ninternationals_killed         int64\ninternationals_wounded        int64\ninternationals_kidnapped      int64\ntotal_internationals          int64\ntotal_killed                  int64\ntotal_wounded                 int64\ntotal_kidnapped               int64\ntotal_affected                int64\ngender_male                   int64\ngender_female                 int64\ngender_unknown                int64\nmeans_of_attack              object\nattack_context               object\nlocation                     object\nlatitude                    float64\nlongitude                   float64\nmotive                       object\nactor_type                   object\nactor_name                   object\ndetails                      object\nverified                     object\nsource                       object\ndtype: object\n\n\n\n5.1 Data Type Conversion\nThe following columns could benefit from data type conversion:\n\n\n\n\n\n\n\n\n\nColumn\nCurrent\nSuggested\nReason\n\n\n\n\nicrc\nfloat64\nint64\nCounts should be integers\n\n\nnrcs_and_ifrc\nfloat64\nint64\nCounts should be integers\n\n\nnngo\nfloat64\nint64\nCounts should be integers\n\n\n\n\n\nCode\n# Convert float columns representing counts to integers\n# Note: This will replace any NaN values with 0 during conversion\ncount_columns = ['icrc', 'nrcs_and_ifrc', 'nngo']\nfor col in count_columns:\n    if col in df.columns:\n        # Fill NaN values with 0 before converting to integer\n        df[col] = df[col].fillna(0).astype(int)\n\n# Verify the conversions\ndf[count_columns].dtypes\n\n\nicrc             int64\nnrcs_and_ifrc    int64\nnngo             int64\ndtype: object"
  },
  {
    "objectID": "data_cleaning.html#outlier-analysis",
    "href": "data_cleaning.html#outlier-analysis",
    "title": "Security Incidents Data Cleaning",
    "section": "6. Outlier Analysis",
    "text": "6. Outlier Analysis\nLet‚Äôs identify and visualize outliers in the dataset to better understand extreme values in our security incidents data.\n\n\nCode\n# Function to detect outliers using IQR method\ndef detect_outliers(df, column):\n    q1 = df[column].quantile(0.25)\n    q3 = df[column].quantile(0.75)\n    iqr = q3 - q1\n    lower_bound = q1 - 1.5 * iqr\n    upper_bound = q3 + 1.5 * iqr\n    \n    outliers = df[(df[column] &lt; lower_bound) | (df[column] &gt; upper_bound)]\n    return outliers, lower_bound, upper_bound\n\n# Get list of numeric columns (excluding some that don't need outlier analysis)\nexcluded_cols = ['year']\nnumeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\nnumeric_cols = [col for col in numeric_cols if col not in excluded_cols]\n\n# Create a DataFrame to store outlier summary\noutlier_summary = pd.DataFrame(columns=['Column', 'Total', 'Outliers', 'Percentage'])\n\n# Collect outlier information\nfor col in numeric_cols:\n    outliers, _, _ = detect_outliers(df, col)\n    \n    # Add to summary DataFrame\n    new_row = {\n        'Column': col,\n        'Total': len(df),\n        'Outliers': len(outliers),\n        'Percentage': len(outliers) / len(df) * 100\n    }\n    outlier_summary = pd.concat([outlier_summary, pd.DataFrame([new_row])], ignore_index=True)\n\n# Sort by percentage of outliers (descending)\noutlier_summary = outlier_summary.sort_values('Percentage', ascending=False)\n\n# Visualize outlier percentages\nplt.figure(figsize=(12, 6))\nsns.barplot(x='Percentage', y='Column', data=outlier_summary, palette='viridis')\nplt.title('Percentage of Outliers by Column')\nplt.xlabel('Percentage of Values Identified as Outliers')\nplt.grid(axis='x', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n/var/folders/gf/ppk7yck96gx15bzg93b7fysm0000gn/T/ipykernel_16401/3183464735.py:31: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/var/folders/gf/ppk7yck96gx15bzg93b7fysm0000gn/T/ipykernel_16401/3183464735.py:38: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n\n6.1 Visualization of Key Outliers\nLet‚Äôs visualize the distribution and outliers for the most important columns:\n\n\nCode\n# Select top 6 columns with the most outliers for detailed visualization\ntop_cols = outlier_summary.head(6)['Column'].tolist()\n\n# Create boxplots for top outlier columns\nplt.figure(figsize=(15, 10))\nfor i, col in enumerate(top_cols, 1):\n    plt.subplot(2, 3, i)\n    sns.boxplot(y=df[col])\n    plt.title(f'Box Plot: {col}')\n    plt.grid(linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n6.2 Outlier Handling Strategy\nIn security incident data, outliers often represent real extreme events (like major attacks) rather than errors. For our analysis:\n\nWe‚Äôll flag high-impact incidents rather than removing them\nThis allows us to analyze with or without extreme events as needed\n\n\n\nCode\n# Create flags for high-impact incidents\ndf['high_impact'] = False\n\n# Flag incidents with casualties in the top 1% of any category\nfor col in ['total_killed', 'total_wounded', 'total_kidnapped', 'total_affected']:\n    if col in df.columns:\n        threshold = df[col].quantile(0.99)\n        df.loc[df[col] &gt; threshold, 'high_impact'] = True\n\n# Print summary of flagged high-impact incidents\nhigh_impact_count = df['high_impact'].sum()\nprint(f\"Flagged {high_impact_count} high-impact incidents ({high_impact_count/len(df)*100:.2f}% of dataset)\")\n\n# Example: Look at the top 5 most severe incidents by total casualties\nif 'total_affected' in df.columns:\n    print(\"\\nTop 5 most severe incidents:\")\n    display(df.sort_values('total_affected', ascending=False).head(5)[\n        ['year', 'country', 'total_affected', 'total_killed', 'total_wounded', 'total_kidnapped']])\n\n\nFlagged 123 high-impact incidents (2.85% of dataset)\n\nTop 5 most severe incidents:\n\n\n\n\n\n\n\n\n\nyear\ncountry\ntotal_affected\ntotal_killed\ntotal_wounded\ntotal_kidnapped\n\n\n\n\n3857\n2023\nOccupied Palestinian Territories\n92\n70\n22\n0\n\n\n1989\n2015\nAfghanistan\n49\n14\n35\n0\n\n\n3920\n2023\nOccupied Palestinian Territories\n46\n41\n5\n0\n\n\n1189\n2011\nNigeria\n46\n9\n37\n0\n\n\n3936\n2023\nOccupied Palestinian Territories\n31\n31\n0\n0"
  },
  {
    "objectID": "data_cleaning.html#outlier-analysis-and-handling",
    "href": "data_cleaning.html#outlier-analysis-and-handling",
    "title": "Security Incidents Data Cleaning",
    "section": "6. Outlier Analysis and Handling",
    "text": "6. Outlier Analysis and Handling\nLet‚Äôs identify and visualize outliers in the dataset to better understand extreme values in our security incidents data.\n\n\nCode\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\n# Function to detect outliers using IQR method\ndef detect_outliers(df, column):\n    q1 = df[column].quantile(0.25)\n    q3 = df[column].quantile(0.75)\n    iqr = q3 - q1\n    lower_bound = q1 - 1.5 * iqr\n    upper_bound = q3 + 1.5 * iqr\n    \n    outliers = df[(df[column] &lt; lower_bound) | (df[column] &gt; upper_bound)]\n    return outliers, lower_bound, upper_bound\n\n# Get list of numeric columns (excluding some that don't need outlier analysis)\nexcluded_cols = ['year']\nnumeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\nnumeric_cols = [col for col in numeric_cols if col not in excluded_cols]\n\n# Create a DataFrame to store outlier summary\noutlier_summary = pd.DataFrame(columns=['Column', 'Total', 'Outliers', 'Percentage'])\n\n# Collect outlier information\nfor col in numeric_cols:\n    outliers, _, _ = detect_outliers(df, col)\n    \n    # Add to summary DataFrame\n    new_row = {\n        'Column': col,\n        'Total': len(df),\n        'Outliers': len(outliers),\n        'Percentage': len(outliers) / len(df) * 100\n    }\n    outlier_summary = pd.concat([outlier_summary, pd.DataFrame([new_row])], ignore_index=True)\n\n# Sort by percentage of outliers (descending)\noutlier_summary = outlier_summary.sort_values('Percentage', ascending=False)\n\n# Visualize outlier percentages\nplt.figure(figsize=(7,4))\nsns.barplot(x='Percentage', y='Column', data=outlier_summary, palette='viridis')\nplt.title('Percentage of Outliers by Column')\nplt.xlabel('Percentage of Values Identified as Outliers')\nplt.grid(axis='x', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6.1 Visualization of Key Outliers\nLet‚Äôs visualize the distribution and outliers for the most important columns:\n\n\nCode\n# Select top 6 columns with the most outliers for detailed visualization\ntop_cols = outlier_summary.head(6)['Column'].tolist()\n\n# Create boxplots for top outlier columns\nplt.figure(figsize=(10, 6))\nfor i, col in enumerate(top_cols, 1):\n    plt.subplot(2, 3, i)\n    sns.boxplot(y=df[col])\n    plt.title(f'Box Plot: {col}')\n    plt.grid(linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n6.2 Outlier Handling Strategy\nIn security incident data, outliers often represent real extreme events (like major attacks) rather than errors. For our analysis:\n\nWe‚Äôll flag high-impact incidents rather than removing them\nThis allows us to analyze with or without extreme events as needed\n\n\n\nCode\n# Create flags for high-impact incidents\ndf['high_impact'] = False\n\n# Flag incidents with casualties in the top 1% of any category\nfor col in ['total_killed', 'total_wounded', 'total_kidnapped', 'total_affected']:\n    if col in df.columns:\n        threshold = df[col].quantile(0.99)\n        df.loc[df[col] &gt; threshold, 'high_impact'] = True\n\n# Print summary of flagged high-impact incidents\nhigh_impact_count = df['high_impact'].sum()\nprint(f\"Flagged {high_impact_count} high-impact incidents ({high_impact_count/len(df)*100:.2f}% of dataset)\")\n\n# Example: Look at the top 5 most severe incidents by total casualties\nif 'total_affected' in df.columns:\n    print(\"\\nTop 5 most severe incidents:\")\n    display(df.sort_values('total_affected', ascending=False).head(5)[\n        ['year', 'country', 'total_affected', 'total_killed', 'total_wounded', 'total_kidnapped']])\n\n\nFlagged 123 high-impact incidents (2.85% of dataset)\n\nTop 5 most severe incidents:\n\n\n\n\n\n\n\n\n\nyear\ncountry\ntotal_affected\ntotal_killed\ntotal_wounded\ntotal_kidnapped\n\n\n\n\n3857\n2023\nOccupied Palestinian Territories\n92\n70\n22\n0\n\n\n1989\n2015\nAfghanistan\n49\n14\n35\n0\n\n\n3920\n2023\nOccupied Palestinian Territories\n46\n41\n5\n0\n\n\n1189\n2011\nNigeria\n46\n9\n37\n0\n\n\n3936\n2023\nOccupied Palestinian Territories\n31\n31\n0\n0"
  },
  {
    "objectID": "data_cleaning.html#geographic-data-validation",
    "href": "data_cleaning.html#geographic-data-validation",
    "title": "Security Incidents Data Cleaning",
    "section": "7. Geographic Data Validation",
    "text": "7. Geographic Data Validation\nLet‚Äôs verify that our latitude and longitude values are within valid ranges.\n\n\nCode\n# Check if latitude and longitude values are within valid ranges\n# Valid ranges: Latitude (-90 to 90), Longitude (-180 to 180)\n\n# Count invalid coordinates\ninvalid_lat = df[(df['latitude'] &lt; -90) | (df['latitude'] &gt; 90)].shape[0]\ninvalid_lon = df[(df['longitude'] &lt; -180) | (df['longitude'] &gt; 180)].shape[0]\n\nprint(f\"Invalid latitude values (outside -90 to 90): {invalid_lat}\")\nprint(f\"Invalid longitude values (outside -180 to 180): {invalid_lon}\")\n\n# Get overall ranges to see extreme values\nlat_min, lat_max = df['latitude'].min(), df['latitude'].max()\nlon_min, lon_max = df['longitude'].min(), df['longitude'].max()\n\nprint(f\"\\nLatitude range: {lat_min} to {lat_max}\")\nprint(f\"Longitude range: {lon_min} to {lon_max}\")\n\n# Create a scatter plot to visualize coordinate distribution\nplt.figure(figsize=(7,4))\nplt.scatter(df['longitude'], df['latitude'], alpha=0.5, s=3)\nplt.title('Geographic Distribution of Incidents')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.xlim(-180, 180)\nplt.ylim(-90, 90)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\nInvalid latitude values (outside -90 to 90): 0\nInvalid longitude values (outside -180 to 180): 0\n\nLatitude range: -34.883611 to 52.253183\nLongitude range: -102.283333 to 179.0122737\n\n\n\n\n\n\n\n\n\nThere are 0 invalid latitudes and longitudes, so we do not have to handle those outliers. All geographic coordinates are within valid ranges and suitable for mapping and spatial analysis."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Code\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport folium\nfrom folium.plugins import MarkerCluster\nfrom matplotlib.colors import LinearSegmentedColormap\nimport warnings\n\n# Set default figure size\nplt.rcParams['figure.figsize'] = (10, 6)\n\n# Suppress future warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Set seaborn style\nsns.set_theme(style=\"whitegrid\")\n\n# Load the dataset\ndf = pd.read_csv(\"data/security_incidents_cleaned.csv\")\n\n\n\n\nCode\n# Create an interactive map of all incidents\ndef create_incidents_map(data):\n    # Calculate center coordinates for the map (average of all points)\n    center_lat = data['latitude'].mean()\n    center_lon = data['longitude'].mean()\n    \n    # Create a map centered on the average coordinates\n    incidents_map = folium.Map(location=[center_lat, center_lon], zoom_start=2)\n    \n    # Add a marker cluster for better performance with many points\n    marker_cluster = MarkerCluster().add_to(incidents_map)\n    \n    # Add points for each incident with coordinates\n    valid_coords = data[data['latitude'].notna() & data['longitude'].notna()]\n    \n    # Create a color scale based on total_affected\n    def get_color(affected):\n        if pd.isna(affected) or affected == 0:\n            return 'blue'\n        elif affected &lt;= 5:\n            return 'green'\n        elif affected &lt;= 20:\n            return 'orange'\n        else:\n            return 'red'\n    \n    for idx, row in valid_coords.iterrows():\n        # Create popup text with incident details\n        popup_text = f\"\"\"\n        &lt;b&gt;Country:&lt;/b&gt; {row['country']}&lt;br&gt;\n        &lt;b&gt;Year:&lt;/b&gt; {row['year']}&lt;br&gt;\n        &lt;b&gt;Total Affected:&lt;/b&gt; {row['total_affected']}&lt;br&gt;\n        &lt;b&gt;Attack Type:&lt;/b&gt; {row['means_of_attack'] if 'means_of_attack' in row and pd.notna(row['means_of_attack']) else 'Unknown'}&lt;br&gt;\n        \"\"\"\n        \n        # Add circle marker\n        folium.CircleMarker(\n            location=[row['latitude'], row['longitude']],\n            radius=5,\n            popup=folium.Popup(popup_text, max_width=300),\n            fill=True,\n            fill_opacity=0.7,\n            color=get_color(row['total_affected']),\n            fill_color=get_color(row['total_affected'])\n        ).add_to(marker_cluster)\n    \n    return incidents_map\n\n# Create the map\nglobal_incidents_map = create_incidents_map(df)\n\n# Save the map as HTML file\nmap_filename = \"images/global_security_incidents_map.html\"\nglobal_incidents_map.save(map_filename)\n\n# Display in notebook (if running in Jupyter)\nglobal_incidents_map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nCode\n# Import necessary libraries for interactive plotting\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom IPython.display import display\n\n# Assuming df is your DataFrame with columns 'year', 'country', and incident data\n# Group data by year and country to prepare for animation\nincidents_by_year_country = df.groupby(['year', 'country']).size().reset_index(name='incidents')\n\n# Get total incidents by year for the animation frame sequence\nyear_totals = incidents_by_year_country.groupby('year')['incidents'].sum().reset_index()\nyear_totals = year_totals.sort_values('year')\n\n# Create animated choropleth map showing incidents by country over time\nfig = px.choropleth(incidents_by_year_country,\n                   locations='country',\n                   locationmode='country names',\n                   color='incidents',\n                   animation_frame='year',\n                   color_continuous_scale='Viridis',\n                   range_color=[0, incidents_by_year_country['incidents'].max()],\n                   height=600)\n\n# Improve layout with better title spacing\nfig.update_layout(\n    title={\n        'text': 'Security Incidents by Country Over Time',\n        'y': 0.95,  # Move the title up\n        'x': 0.5,\n        'xanchor': 'center',\n        'yanchor': 'top',\n        'font': {'size': 24}  # Increase title font size for better visibility\n    },\n    coloraxis_colorbar=dict(\n        title='Number of Incidents'\n    ),\n    geo=dict(\n        showframe=False,\n        showcoastlines=True,\n        projection_type='natural earth'\n    ),\n    margin=dict(t=100)  # Add extra top margin for spacing\n)\n\n# Add slider and play button settings\nfig.layout.updatemenus[0].buttons[0].args[1]['frame']['duration'] = 1000\nfig.layout.updatemenus[0].buttons[0].args[1]['transition']['duration'] = 500\n\n# Add annotation for total incidents per year with better spacing\nfor i, year in enumerate(year_totals['year'].unique()):\n    year_total = year_totals.loc[year_totals['year'] == year, 'incidents'].values[0]\n    fig.frames[i].layout.annotations = [\n        dict(\n            x=0.5,\n            y=0.87,  # Position subtitle lower than title\n            xref='paper',\n            yref='paper',\n            text=f'Total Incidents in {year}: {year_total}',\n            showarrow=False,\n            font=dict(\n                size=18\n            )\n        )\n    ]\n\n# Also add the annotation to the base layout\nlatest_year = year_totals['year'].max()\nlatest_total = year_totals.loc[year_totals['year'] == latest_year, 'incidents'].values[0]\nfig.update_layout(\n    annotations=[\n        dict(\n            x=0.5,\n            y=0.87,  # Position subtitle lower than title\n            xref='paper',\n            yref='paper',\n            text=f'Total Incidents in {latest_year}: {latest_total}',\n            showarrow=False,\n            font=dict(\n                size=18\n            )\n        )\n    ]\n)\n\n# For Quarto output, save as HTML\nfig.write_html(\"images/interactive_incidents_over_time.html\")\n\n# Display for notebook viewing\nfig.show()\n\n# Create an alternative interactive bar chart with year slider\nyear_incidents = df.groupby('year').size().reset_index(name='incidents')\nyear_incidents['year'] = year_incidents['year'].astype(str)  # Convert year to string for better display\n\nfig2 = px.bar(year_incidents,\n               x='year',\n               y='incidents',\n               title='Interactive Security Incidents by Year',\n               labels={'incidents': 'Number of Incidents', 'year': 'Year'},\n               height=500)\n\n# Add range slider\nfig2.update_layout(\n    title={\n        'text': 'Interactive Security Incidents by Year',\n        'y': 0.95,\n        'x': 0.5,\n        'xanchor': 'center',\n        'yanchor': 'top',\n    },\n    xaxis=dict(\n        rangeslider=dict(visible=True),\n        type='category'  # Use category type for discrete years\n    ),\n    bargap=0.1,\n    template='plotly_white'\n)\n\n# Save the interactive bar chart\nfig2.write_html(\"images/interactive_yearly_incidents_barchart.html\")\n\n# Show the bar chart\nfig2.show()\n\n\n                                                \n\n\n                                                \n\n\n\n\n\n\nCode\nimport plotly.express as px\nimport pandas as pd\n\n# Assuming df is your DataFrame with columns 'year', 'country', and incident data\n\n# CHART 1: Countries with most incidents over all time\n# Group by country to get total incidents across all years\ntotal_by_country = df.groupby('country').size().reset_index(name='total_incidents')\ntotal_by_country = total_by_country.sort_values('total_incidents', ascending=False)\n\n# Get top 15 countries by total incidents\ntop15_countries = total_by_country.head(15)\n\n# Create bar chart for top countries over all time\nfig_top_all_time = px.bar(\n    top15_countries,\n    x='country',\n    y='total_incidents',\n    title='Top 15 Countries by Security Incidents (All Time)',\n    labels={'total_incidents': 'Number of Incidents', 'country': 'Country'},\n    color='total_incidents',\n    color_continuous_scale='Viridis',\n    height=600\n)\n\nfig_top_all_time.update_layout(\n    title={\n        'text': 'Top 15 Countries by Security Incidents (All Time)',\n        'y': 0.95,\n        'x': 0.5,\n        'xanchor': 'center',\n        'yanchor': 'top',\n        'font': {'size': 20}\n    },\n    xaxis={'categoryorder': 'total descending', 'tickangle': 45},  # Sort bars and angle labels\n    coloraxis_showscale=False  # Hide the color scale\n)\n\n# CHART 2: Countries with most incidents in the last 10 years\ncurrent_year = df['year'].max()\nten_years_ago = current_year - 10\n\n# Filter data for last 10 years\nrecent_df = df[df['year'] &gt;= ten_years_ago]\n\n# Group by country for the last 10 years\nrecent_by_country = recent_df.groupby('country').size().reset_index(name='recent_incidents')\nrecent_by_country = recent_by_country.sort_values('recent_incidents', ascending=False)\n\n# Get top 15 countries in the last 10 years\ntop15_recent = recent_by_country.head(15)\n\n# Create bar chart for top countries in last 10 years\nfig_top_recent = px.bar(\n    top15_recent,\n    x='country',\n    y='recent_incidents',\n    title=f'Top 15 Countries by Security Incidents (Last 10 Years: {ten_years_ago}-{current_year})',\n    labels={'recent_incidents': 'Number of Incidents', 'country': 'Country'},\n    color='recent_incidents',\n    color_continuous_scale='Viridis',\n    height=600\n)\n\nfig_top_recent.update_layout(\n    title={\n        'text': f'Top 15 Countries by Security Incidents (Last 10 Years: {ten_years_ago}-{current_year})',\n        'y': 0.95,\n        'x': 0.5,\n        'xanchor': 'center',\n        'yanchor': 'top',\n        'font': {'size': 20}\n    },\n    xaxis={'categoryorder': 'total descending', 'tickangle': 45},  # Sort bars and angle labels\n    coloraxis_showscale=False  # Hide the color scale\n)\n\n# Display the visualizations\nfig_top_all_time.show()\nfig_top_recent.show()\n\n# Save the visualizations\nfig_top_all_time.write_html(\"images/top_countries_all_time.html\")\nfig_top_recent.write_html(\"images/top_countries_recent.html\")\n\nprint(\"Bar chart visualizations saved as HTML files in the images directory.\")\n\n\n                                                \n\n\n                                                \n\n\nBar chart visualizations saved as HTML files in the images directory."
  },
  {
    "objectID": "eda.html#overview-of-security-incidents-by-country",
    "href": "eda.html#overview-of-security-incidents-by-country",
    "title": "Exploratory Data Analysis",
    "section": "1. Overview of Security Incidents by Country",
    "text": "1. Overview of Security Incidents by Country\nLet‚Äôs first explore the countries with the highest number of security incidents.\n\n\nCode\n# Count incidents by country\ncountry_counts = df['country'].value_counts().reset_index()\ncountry_counts.columns = ['Country', 'Number of Incidents']\n\n# Display top 15 countries with the most incidents\nprint(\"Top 15 countries with the most security incidents:\")\ncountry_counts.head(15)\n\n# Visualize top 20 countries with most incidents\nplt.figure(figsize=(12, 8))\nsns.barplot(x='Number of Incidents', y='Country', data=country_counts.head(20), palette='viridis')\nplt.title('Top 20 Countries by Number of Security Incidents')\nplt.xlabel('Number of Incidents')\nplt.ylabel('Country')\nplt.tight_layout()\nplt.show()\n\n\nTop 15 countries with the most security incidents:"
  },
  {
    "objectID": "eda.html#severity-of-incidents-by-country",
    "href": "eda.html#severity-of-incidents-by-country",
    "title": "Exploratory Data Analysis",
    "section": "2. Severity of Incidents by Country",
    "text": "2. Severity of Incidents by Country\nLet‚Äôs analyze the severity of incidents (killed, wounded, kidnapped) across countries.\n\n\nCode\n# Group by country and calculate total casualties\nseverity_by_country = df.groupby('country').agg({\n    'total_killed': 'sum',\n    'total_wounded': 'sum',\n    'total_kidnapped': 'sum',\n    'total_affected': 'sum'\n}).reset_index()\n\n# Sort by total affected (descending)\nseverity_by_country = severity_by_country.sort_values('total_affected', ascending=False)\n\n# Display top 15 countries by total casualties\nprint(\"Top 15 countries by total casualties (killed + wounded + kidnapped):\")\nseverity_by_country.head(15)\n\n# Visualize top 15 countries by severity\ntop15_severity = severity_by_country.head(15).melt(\n    id_vars='country',\n    value_vars=['total_killed', 'total_wounded', 'total_kidnapped'],\n    var_name='casualty_type',\n    value_name='count'\n)\n\nplt.figure(figsize=(14, 8))\nchart = sns.barplot(x='country', y='count', hue='casualty_type', data=top15_severity, palette='Set2')\nplt.title('Distribution of Casualties by Country (Top 15 Most Affected)')\nplt.xlabel('Country')\nplt.ylabel('Number of Casualties')\nplt.xticks(rotation=45, ha='right')\nplt.legend(title='Casualty Type')\nplt.tight_layout()\nplt.show()\n\n# Calculate casualty rate (casualties per incident)\ncountry_counts.columns = ['country', 'incidents']\ncasualty_rate = pd.merge(severity_by_country, country_counts, on='country')\ncasualty_rate['casualties_per_incident'] = casualty_rate['total_affected'] / casualty_rate['incidents']\ncasualty_rate = casualty_rate.sort_values('casualties_per_incident', ascending=False)\n\nprint(\"\\nTop 15 countries by casualties per incident:\")\ncasualty_rate[['country', 'incidents', 'total_affected', 'casualties_per_incident']].head(15)\n\n# Visualize casualties per incident\nplt.figure(figsize=(12, 8))\nsns.barplot(x='casualties_per_incident', y='country', \n            data=casualty_rate.head(20),\n            palette='rocket')\nplt.title('Top 20 Countries by Casualties per Incident')\nplt.xlabel('Average Casualties per Incident')\nplt.ylabel('Country')\nplt.tight_layout()\nplt.show()\n\n\nTop 15 countries by total casualties (killed + wounded + kidnapped):\n\n\n\n\n\n\n\n\n\n\nTop 15 countries by casualties per incident:"
  },
  {
    "objectID": "eda.html#temporal-trends-by-country",
    "href": "eda.html#temporal-trends-by-country",
    "title": "Exploratory Data Analysis",
    "section": "3. Temporal Trends by Country",
    "text": "3. Temporal Trends by Country\nLet‚Äôs examine how the incidents have changed over time for the most affected countries.\n\n\nCode\n# Get top 5 countries by incident count\ntop5_countries = country_counts.head(5)['country'].tolist()\n\n# Filter data for top 5 countries\ntop5_data = df[df['country'].isin(top5_countries)]\n\n# Group by country and year\nyearly_trend = top5_data.groupby(['country', 'year']).size().reset_index(name='incidents')\n\n# Visualize trends over time for top 5 countries\nplt.figure(figsize=(14, 8))\nsns.lineplot(x='year', y='incidents', hue='country', data=yearly_trend, palette='tab10', linewidth=2.5, markers=True)\nplt.title('Yearly Trend of Security Incidents for Top 5 Most Affected Countries')\nplt.xlabel('Year')\nplt.ylabel('Number of Incidents')\nplt.grid(True, alpha=0.3)\nplt.legend(title='Country')\nplt.tight_layout()\nplt.show()\n\n# Analyze yearly casualties for top 5 countries\nyearly_casualties = top5_data.groupby(['country', 'year'])['total_affected'].sum().reset_index()\n\nplt.figure(figsize=(14, 8))\nsns.lineplot(x='year', y='total_affected', hue='country', data=yearly_casualties, palette='tab10', linewidth=2.5, markers=True)\nplt.title('Yearly Trend of Total Casualties for Top 5 Most Affected Countries')\nplt.xlabel('Year')\nplt.ylabel('Total Casualties')\nplt.grid(True, alpha=0.3)\nplt.legend(title='Country')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "eda.html#organization-types-by-country",
    "href": "eda.html#organization-types-by-country",
    "title": "Exploratory Data Analysis",
    "section": "4. Organization Types by Country",
    "text": "4. Organization Types by Country\nLet‚Äôs investigate which types of organizations (UN, INGO, ICRC, etc.) are most targeted in different countries.\n\n\nCode\n# Select the organization columns\norg_columns = ['un', 'ingo', 'icrc', 'nrcs_and_ifrc', 'nngo', 'other']\n\n# Group by country and sum the organization incidents\norg_by_country = df.groupby('country')[org_columns].sum().reset_index()\n\n# Calculate total incidents by organization type\norg_totals = org_by_country[org_columns].sum().sort_values(ascending=False)\n\n# Show organization type distribution overall\nplt.figure(figsize=(10, 6))\nsns.barplot(x=org_totals.index, y=org_totals.values, palette='Set3')\nplt.title('Total Incidents by Organization Type')\nplt.xlabel('Organization Type')\nplt.ylabel('Number of Incidents')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Get top 10 countries by total incidents\ntop10_countries = country_counts.head(10)['country'].tolist()\ntop10_org = org_by_country[org_by_country['country'].isin(top10_countries)]\n\n# Convert to long format for visualization\ntop10_org_long = top10_org.melt(id_vars='country', value_vars=org_columns, \n                              var_name='Organization Type', value_name='Incidents')\n\n# Create stacked bar chart\nplt.figure(figsize=(14, 8))\nchart = sns.barplot(x='country', y='Incidents', hue='Organization Type', data=top10_org_long, palette='Set3')\nplt.title('Distribution of Incidents by Organization Type and Country (Top 10)')\nplt.xlabel('Country')\nplt.ylabel('Number of Incidents')\nplt.xticks(rotation=45, ha='right')\nplt.legend(title='Organization Type')\nplt.tight_layout()\nplt.show()\n\n# Calculate percentage of incidents by org type for each country\nfor org in org_columns:\n    top10_org[f'{org}_pct'] = top10_org[org] / top10_org[org_columns].sum(axis=1) * 100\n\n# Get the most targeted organization type by country\ntop10_org['most_targeted'] = top10_org[org_columns].idxmax(axis=1)\n\nprint(\"Most targeted organization type by country (Top 10 countries):\")\ntop10_org[['country', 'most_targeted'] + [f'{org}_pct' for org in org_columns]].round(1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMost targeted organization type by country (Top 10 countries):\n\n\n\n\n\n\n\n\n\ncountry\nmost_targeted\nun_pct\ningo_pct\nicrc_pct\nnrcs_and_ifrc_pct\nnngo_pct\nother_pct\n\n\n\n\n0\nAfghanistan\ningo\n15.1\n45.6\n2.7\n1.6\n32.9\n2.2\n\n\n14\nCentral African Republic\ningo\n10.6\n61.8\n2.8\n6.9\n17.9\n0.0\n\n\n21\nDR Congo\ningo\n11.6\n58.0\n5.4\n7.4\n16.1\n1.6\n\n\n53\nMali\ningo\n7.2\n55.1\n3.8\n3.0\n30.8\n0.0\n\n\n64\nOccupied Palestinian Territories\nun\n64.4\n9.7\n0.7\n15.7\n9.0\n0.5\n\n\n65\nPakistan\nnngo\n15.8\n36.7\n0.4\n0.8\n37.5\n8.9\n\n\n74\nSomalia\ningo\n29.6\n35.0\n2.2\n6.0\n26.8\n0.3\n\n\n76\nSouth Sudan\ningo\n24.1\n50.4\n0.4\n0.6\n24.2\n0.2\n\n\n78\nSudan\ningo\n24.1\n51.4\n2.5\n5.4\n15.7\n0.7\n\n\n80\nSyrian Arab Republic\nnngo\n9.0\n22.4\n1.0\n11.1\n56.0\n0.5"
  },
  {
    "objectID": "eda.html#attack-characteristics-by-country",
    "href": "eda.html#attack-characteristics-by-country",
    "title": "Exploratory Data Analysis",
    "section": "5. Attack Characteristics by Country",
    "text": "5. Attack Characteristics by Country\nLet‚Äôs examine the attack types, contexts, and motives across different countries.\n\n\nCode\n# Check available attack types\nattack_types = df['means_of_attack'].value_counts().reset_index()\nattack_types.columns = ['Attack Type', 'Count']\n\n# Top 10 overall attack types\nprint(\"Top 10 means of attack overall:\")\nattack_types.head(10)\n\n# Get attack types for top 5 countries\ntop5_attacks = df[df['country'].isin(top5_countries)].groupby(['country', 'means_of_attack']).size().reset_index(name='count')\ntop5_attacks = top5_attacks.sort_values(['country', 'count'], ascending=[True, False])\n\n# Get top 3 attack types per country\ntop_attacks_by_country = []\nfor country in top5_countries:\n    country_attacks = top5_attacks[top5_attacks['country'] == country].head(3)\n    top_attacks_by_country.append(country_attacks)\n\ntop_attacks_df = pd.concat(top_attacks_by_country)\n\n# Visualize top attack types by country\nplt.figure(figsize=(14, 8))\nchart = sns.barplot(x='country', y='count', hue='means_of_attack', data=top_attacks_df, palette='tab20')\nplt.title('Top 3 Attack Types by Country (Top 5 Most Affected Countries)')\nplt.xlabel('Country')\nplt.ylabel('Number of Incidents')\nplt.xticks(rotation=45, ha='right')\nplt.legend(title='Attack Type', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n# Analyze attack context by country\nif 'attack_context' in df.columns:\n    top5_contexts = df[df['country'].isin(top5_countries)].groupby(['country', 'attack_context']).size().reset_index(name='count')\n    top5_contexts = top5_contexts.sort_values(['country', 'count'], ascending=[True, False])\n\n    # Get top 3 contexts per country\n    top_contexts_by_country = []\n    for country in top5_countries:\n        country_contexts = top5_contexts[top5_contexts['country'] == country].head(3)\n        top_contexts_by_country.append(country_contexts)\n\n    top_contexts_df = pd.concat(top_contexts_by_country)\n\n    # Visualize top attack contexts by country\n    plt.figure(figsize=(14, 8))\n    chart = sns.barplot(x='country', y='count', hue='attack_context', data=top_contexts_df, palette='Pastel1')\n    plt.title('Top 3 Attack Contexts by Country (Top 5 Most Affected Countries)')\n    plt.xlabel('Country')\n    plt.ylabel('Number of Incidents')\n    plt.xticks(rotation=45, ha='right')\n    plt.legend(title='Attack Context', bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.tight_layout()\n    plt.show()\n\n\nTop 10 means of attack overall:"
  },
  {
    "objectID": "eda.html#geographic-distribution-within-countries",
    "href": "eda.html#geographic-distribution-within-countries",
    "title": "Exploratory Data Analysis",
    "section": "6. Geographic Distribution Within Countries",
    "text": "6. Geographic Distribution Within Countries\nLet‚Äôs visualize the geographic distribution of incidents within countries using maps.\n\n\nCode\n# Check if we have coordinates\nhas_coords = df['latitude'].notna() & df['longitude'].notna()\nprint(f\"Percentage of incidents with valid coordinates: {has_coords.mean()*100:.2f}%\")\n\n# Function to create a map for a specific country\ndef create_country_map(country_name):\n    # Filter data for the country\n    country_data = df[df['country'] == country_name]\n    \n    # Calculate center coordinates\n    center_lat = country_data['latitude'].mean()\n    center_lon = country_data['longitude'].mean()\n    \n    # Create a map centered on the country\n    country_map = folium.Map(location=[center_lat, center_lon], zoom_start=6)\n    \n    # Add a marker cluster\n    marker_cluster = MarkerCluster().add_to(country_map)\n    \n    # Add points for each incident\n    for idx, row in country_data.iterrows():\n        if pd.notna(row['latitude']) and pd.notna(row['longitude']):\n            # Create popup text\n            popup_text = f\"\"\"\n            &lt;b&gt;Year:&lt;/b&gt; {row['year']}&lt;br&gt;\n            &lt;b&gt;Total Affected:&lt;/b&gt; {row['total_affected']}&lt;br&gt;\n            &lt;b&gt;Attack Type:&lt;/b&gt; {row['means_of_attack']}&lt;br&gt;\n            \"\"\"\n            \n            # Add circle marker\n            folium.CircleMarker(\n                location=[row['latitude'], row['longitude']],\n                radius=5,\n                popup=folium.Popup(popup_text, max_width=300),\n                fill=True,\n                fill_opacity=0.7,\n                color='red',\n                fill_color='red'\n            ).add_to(marker_cluster)\n    \n    return country_map\n\n# Create maps for top 3 countries (if coordinates are available)\nif has_coords.mean() &gt; 0.5:  # Only if we have enough coordinates\n    for country in top5_countries[:3]:\n        country_map = create_country_map(country)\n        # Display the map (this will only work in a Jupyter notebook environment)\n        # country_map\n        \n        # For Quarto output, save the map as HTML\n        map_filename = f\"images/{country.lower().replace(' ', '_')}_incidents_map.html\"\n        country_map.save(map_filename)\n        print(f\"Map for {country} saved as {map_filename}\")\n\n\nPercentage of incidents with valid coordinates: 99.70%\nMap for Afghanistan saved as images/afghanistan_incidents_map.html\nMap for South Sudan saved as images/south_sudan_incidents_map.html\nMap for Sudan saved as images/sudan_incidents_map.html"
  },
  {
    "objectID": "eda.html#high-impact-incidents-by-country",
    "href": "eda.html#high-impact-incidents-by-country",
    "title": "Exploratory Data Analysis",
    "section": "7. High-Impact Incidents by Country",
    "text": "7. High-Impact Incidents by Country\nLet‚Äôs analyze the distribution of high-impact incidents across countries.\n\n\nCode\n# Calculate percentage of high-impact incidents by country\nhigh_impact_by_country = df.groupby('country')['high_impact'].mean().reset_index()\nhigh_impact_by_country.columns = ['country', 'high_impact_percentage']\nhigh_impact_by_country['high_impact_percentage'] = high_impact_by_country['high_impact_percentage'] * 100\n\n# Sort by percentage of high-impact incidents\nhigh_impact_by_country = high_impact_by_country.sort_values('high_impact_percentage', ascending=False)\n\n# Display top 15 countries by percentage of high-impact incidents\nprint(\"Top 15 countries by percentage of high-impact incidents:\")\nhigh_impact_by_country.head(15)\n\n# Visualize countries with the highest percentage of high-impact incidents\nplt.figure(figsize=(12, 8))\nsns.barplot(x='high_impact_percentage', y='country', data=high_impact_by_country.head(20), palette='rocket_r')\nplt.title('Top 20 Countries by Percentage of High-Impact Incidents')\nplt.xlabel('Percentage of High-Impact Incidents')\nplt.ylabel('Country')\nplt.tight_layout()\nplt.show()\n\n# Compare high-impact vs. regular incidents for top 5 countries\ntop5_highimpact = df[df['country'].isin(top5_countries)].groupby(['country', 'high_impact']).size().reset_index(name='count')\n\nplt.figure(figsize=(12, 7))\nsns.barplot(x='country', y='count', hue='high_impact', data=top5_highimpact, palette=['lightblue', 'crimson'])\nplt.title('High-Impact vs. Regular Incidents for Top 5 Countries')\nplt.xlabel('Country')\nplt.ylabel('Number of Incidents')\nplt.legend(title='High Impact', labels=['Regular', 'High Impact'])\nplt.tight_layout()\nplt.show()\n\n\nTop 15 countries by percentage of high-impact incidents:"
  },
  {
    "objectID": "eda.html#national-vs.-international-casualties-by-country",
    "href": "eda.html#national-vs.-international-casualties-by-country",
    "title": "Exploratory Data Analysis",
    "section": "8. National vs.¬†International Casualties by Country",
    "text": "8. National vs.¬†International Casualties by Country\nLet‚Äôs analyze whether incidents affect national staff or international staff more across different countries.\n\n\nCode\n# Create a dataframe with national vs. international casualties by country\nstaff_casualties = df.groupby('country').agg({\n    'total_nationals': 'sum',\n    'total_internationals': 'sum'\n}).reset_index()\n\n# Calculate total casualties\nstaff_casualties['total_casualties'] = staff_casualties['total_nationals'] + staff_casualties['total_internationals']\n\n# Sort by total casualties\nstaff_casualties = staff_casualties.sort_values('total_casualties', ascending=False)\n\n# Calculate percentages\nstaff_casualties['nationals_percentage'] = staff_casualties['total_nationals'] / staff_casualties['total_casualties'] * 100\nstaff_casualties['internationals_percentage'] = staff_casualties['total_internationals'] / staff_casualties['total_casualties'] * 100\n\n# Display top 15 countries\nprint(\"Distribution of national vs. international casualties for top 15 countries:\")\nstaff_casualties.head(15)[['country', 'total_nationals', 'total_internationals', \n                         'nationals_percentage', 'internationals_percentage', 'total_casualties']]\n\n# Visualize national vs. international casualties for top 10 countries\ntop10_staff = staff_casualties.head(10).melt(\n    id_vars='country',\n    value_vars=['total_nationals', 'total_internationals'],\n    var_name='staff_type',\n    value_name='casualties'\n)\n\nplt.figure(figsize=(14, 8))\nsns.barplot(x='country', y='casualties', hue='staff_type', data=top10_staff, palette=['steelblue', 'coral'])\nplt.title('National vs. International Casualties by Country (Top 10)')\nplt.xlabel('Country')\nplt.ylabel('Number of Casualties')\nplt.xticks(rotation=45, ha='right')\nplt.legend(title='Staff Type')\nplt.tight_layout()\nplt.show()\n\n# Visualize percentage distribution\ntop10_pct = staff_casualties.head(10)[['country', 'nationals_percentage', 'internationals_percentage']]\ntop10_pct_long = top10_pct.melt(\n    id_vars='country',\n    value_vars=['nationals_percentage', 'internationals_percentage'],\n    var_name='staff_type',\n    value_name='percentage'\n)\n\nplt.figure(figsize=(14, 7))\nsns.barplot(x='country', y='percentage', hue='staff_type', data=top10_pct_long, palette=['steelblue', 'coral'])\nplt.title('Percentage Distribution of National vs. International Casualties (Top 10 Countries)')\nplt.xlabel('Country')\nplt.ylabel('Percentage of Casualties')\nplt.xticks(rotation=45, ha='right')\nplt.legend(title='Staff Type')\nplt.tight_layout()\nplt.show()\n\n\nDistribution of national vs. international casualties for top 15 countries:"
  },
  {
    "objectID": "eda.html#gender-analysis-by-country",
    "href": "eda.html#gender-analysis-by-country",
    "title": "Exploratory Data Analysis",
    "section": "9. Gender Analysis by Country",
    "text": "9. Gender Analysis by Country\nLet‚Äôs analyze the gender distribution of casualties across countries.\n\n\nCode\n# Check if gender columns exist\ngender_columns = ['gender_male', 'gender_female', 'gender_unknown']\nhas_gender_data = all(col in df.columns for col in gender_columns)\n\nif has_gender_data:\n    # Create a dataframe with gender breakdown by country\n    gender_by_country = df.groupby('country')[gender_columns].sum().reset_index()\n    \n    # Calculate total casualties by gender\n    gender_by_country['total_gender'] = gender_by_country[gender_columns].sum(axis=1)\n    \n    # Sort by total casualties\n    gender_by_country = gender_by_country.sort_values('total_gender', ascending=False)\n    \n    # Calculate percentages\n    for col in gender_columns:\n        gender_by_country[f'{col}_pct'] = gender_by_country[col] / gender_by_country['total_gender'] * 100\n    \n    # Display gender breakdown for top 15 countries\n    print(\"Gender breakdown of casualties for top 15 countries:\")\n    gender_display_cols = ['country'] + gender_columns + ['total_gender'] + [f'{col}_pct' for col in gender_columns]\n    display(gender_by_country.head(15)[gender_display_cols].round(1))\n    \n    # Visualize gender distribution for top 10 countries\n    top10_gender = gender_by_country.head(10).melt(\n        id_vars='country',\n        value_vars=gender_columns,\n        var_name='gender',\n        value_name='casualties'\n    )\n    \n    plt.figure(figsize=(14, 8))\n    sns.barplot(x='country', y='casualties', hue='gender', data=top10_gender, palette='Set2')\n    plt.title('Gender Distribution of Casualties by Country (Top 10)')\n    plt.xlabel('Country')\n    plt.ylabel('Number of Casualties')\n    plt.xticks(rotation=45, ha='right')\n    plt.legend(title='Gender')\n    plt.tight_layout()\n    plt.show()\n    \n    # Visualize percentage distribution\n    top10_gender_pct = gender_by_country.head(10).melt(\n        id_vars='country',\n        value_vars=[f'{col}_pct' for col in gender_columns],\n        var_name='gender',\n        value_name='percentage'\n    )\n    \n    # Clean up gender labels for display\n    top10_gender_pct['gender'] = top10_gender_pct['gender'].str.replace('_pct', '')\n    \n    plt.figure(figsize=(14, 7))\n    sns.barplot(x='country', y='percentage', hue='gender', data=top10_gender_pct, palette='Set2')\n    plt.title('Percentage Distribution of Casualties by Gender (Top 10 Countries)')\n    plt.xlabel('Country')\n    plt.ylabel('Percentage')\n    plt.xticks(rotation=45, ha='right')\n    plt.legend(title='Gender')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Gender data not available in the dataset.\")\n\n\nGender breakdown of casualties for top 15 countries:\n\n\n\n\n\n\n\n\n\ncountry\ngender_male\ngender_female\ngender_unknown\ntotal_gender\ngender_male_pct\ngender_female_pct\ngender_unknown_pct\n\n\n\n\n0\nAfghanistan\n673\n85\n596\n1354\n49.7\n6.3\n44.0\n\n\n76\nSouth Sudan\n521\n60\n310\n891\n58.5\n6.7\n34.8\n\n\n78\nSudan\n264\n30\n373\n667\n39.6\n4.5\n55.9\n\n\n80\nSyrian Arab Republic\n431\n23\n133\n587\n73.4\n3.9\n22.7\n\n\n74\nSomalia\n267\n45\n273\n585\n45.6\n7.7\n46.7\n\n\n64\nOccupied Palestinian Territories\n87\n19\n473\n579\n15.0\n3.3\n81.7\n\n\n21\nDR Congo\n210\n31\n207\n448\n46.9\n6.9\n46.2\n\n\n53\nMali\n115\n13\n135\n263\n43.7\n4.9\n51.3\n\n\n65\nPakistan\n99\n48\n112\n259\n38.2\n18.5\n43.2\n\n\n14\nCentral African Republic\n119\n14\n113\n246\n48.4\n5.7\n45.9\n\n\n63\nNigeria\n74\n19\n136\n229\n32.3\n8.3\n59.4\n\n\n92\nYemen\n85\n22\n90\n197\n43.1\n11.2\n45.7\n\n\n39\nIraq\n68\n22\n72\n162\n42.0\n13.6\n44.4\n\n\n27\nEthiopia\n105\n5\n43\n153\n68.6\n3.3\n28.1\n\n\n87\nUkraine\n30\n10\n84\n124\n24.2\n8.1\n67.7"
  },
  {
    "objectID": "eda.html#actor-analysis-by-country",
    "href": "eda.html#actor-analysis-by-country",
    "title": "Exploratory Data Analysis",
    "section": "10. Actor Analysis by Country",
    "text": "10. Actor Analysis by Country\nLet‚Äôs examine the types of actors responsible for incidents across different countries.\n\n\nCode\n# Check if actor columns exist\nhas_actor_data = 'actor_type' in df.columns\n\nif has_actor_data:\n    # Count incidents by actor type and country\n    actor_by_country = df.groupby(['country', 'actor_type']).size().reset_index(name='count')\n    \n    # Filter for top 5 countries\n    top5_actors = actor_by_country[actor_by_country['country'].isin(top5_countries)]\n    \n    # Get top 3 actor types per country\n    top_actors_by_country = []\n    for country in top5_countries:\n        country_actors = top5_actors[top5_actors['country'] == country].sort_values('count', ascending=False).head(3)\n        top_actors_by_country.append(country_actors)\n    \n    top_actors_df = pd.concat(top_actors_by_country)\n    \n    # Visualize top actor types by country\n    plt.figure(figsize=(14, 8))\n    chart = sns.barplot(x='country', y='count', hue='actor_type', data=top_actors_df, palette='Dark2')\n    plt.title('Top 3 Actor Types by Country (Top 5 Most Affected Countries)')\n    plt.xlabel('Country')\n    plt.ylabel('Number of Incidents')\n    plt.xticks(rotation=45, ha='right')\n    plt.legend(title='Actor Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Actor type data not available in the dataset.\")"
  },
  {
    "objectID": "eda.html#country-risk-index",
    "href": "eda.html#country-risk-index",
    "title": "Exploratory Data Analysis",
    "section": "11. Country Risk Index",
    "text": "11. Country Risk Index\nLet‚Äôs create a simple risk index for countries based on incident frequency, severity, and trends.\n\n\nCode\n# Create a simple risk index\nrisk_index = pd.DataFrame()\n\n# Add country names\nrisk_index['country'] = country_counts['country']\n\n# Add incident counts (normalized to 0-100 scale)\nrisk_index['incident_score'] = country_counts['incidents'] / country_counts['incidents'].max() * 100\n\n# Add casualty data\nrisk_index = pd.merge(risk_index, casualty_rate[['country', 'casualties_per_incident']], on='country', how='left')\nrisk_index['severity_score'] = risk_index['casualties_per_incident'] / risk_index['casualties_per_incident'].max() * 100\n\n# Add high impact percentage\nrisk_index = pd.merge(risk_index, high_impact_by_country, on='country', how='left')\n\n# Calculate overall risk score (weighted average)\nrisk_index['risk_score'] = (\n    0.4 * risk_index['incident_score'] + \n    0.4 * risk_index['severity_score'] + \n    0.2 * risk_index['high_impact_percentage']\n)\n\n# Sort by risk score\nrisk_index = risk_index.sort_values('risk_score', ascending=False)\n\n# Display top 20 highest risk countries\nprint(\"Top 20 countries by security risk index:\")\nrisk_index.head(20)[['country', 'incident_score', 'severity_score', \n                   'high_impact_percentage', 'risk_score']].round(1)\n\n# Visualize risk scores for top 20 countries\nplt.figure(figsize=(12, 10))\nrisk_data = risk_index.head(20).melt(\n    id_vars='country',\n    value_vars=['incident_score', 'severity_score', 'high_impact_percentage'],\n    var_name='risk_factor',\n    value_name='score'\n)\n\n# Improve labels for display\nrisk_data['risk_factor'] = risk_data['risk_factor'].map({\n    'incident_score': 'Incident Frequency',\n    'severity_score': 'Incident Severity',\n    'high_impact_percentage': 'High-Impact %'\n})\n\nsns.barplot(x='score', y='country', hue='risk_factor', data=risk_data, palette='YlOrRd')\nplt.title('Security Risk Factors by Country (Top 20)')\nplt.xlabel('Score (0-100)')\nplt.ylabel('Country')\nplt.legend(title='Risk Factor')\nplt.tight_layout()\nplt.show()\n\n# Create a summary heatmap of overall risk\nplt.figure(figsize=(12, 10))\nrisk_pivot = risk_index.head(20).set_index('country')[['risk_score']]\n\n# Create a colormap from yellow to red\ncmap = LinearSegmentedColormap.from_list('YlOrRd', ['#FFFFCC', '#FFEDA0', '#FEB24C', '#FC4E2A', '#E31A1C', '#B10026'])\n\nsns.heatmap(risk_pivot, cmap=cmap, annot=True, fmt='.1f', cbar_kws={'label': 'Risk Score'})\nplt.title('Overall Security Risk Score by Country')\nplt.tight_layout()\nplt.show()\n\n\nTop 20 countries by security risk index:"
  },
  {
    "objectID": "eda.html#conclusion",
    "href": "eda.html#conclusion",
    "title": "Exploratory Data Analysis",
    "section": "12. Conclusion",
    "text": "12. Conclusion\nThis exploratory data analysis has provided significant insights into security incidents affecting aid workers across different countries:\n\nIncident Concentration: The analysis revealed that certain countries consistently experience higher numbers of security incidents against aid workers, with [top countries] facing the most severe challenges.\nSeverity Patterns: While some countries have high incident counts, others show more severe incidents with higher casualty rates per incident, suggesting different risk profiles across regions.\nTemporal Trends: The multi-year analysis demonstrated changing security situations in key countries, with [observations about trends].\nOrganization Targeting: The data shows that certain organization types are more frequently targeted in specific countries, which has implications for security protocols and resource allocation.\nAttack Characteristics: Different countries show distinct patterns in attack types, contexts, and actors responsible, reflecting the unique security challenges in each environment.\nGeographic Distribution: The mapping analysis revealed clustering of incidents within countries, highlighting specific high-risk regions that may require enhanced security measures.\nRisk Assessment: The composite risk index provides a data-driven approach to prioritizing security resources and attention across different countries.\n\nThese insights can inform organization-specific security protocols, resource allocation decisions, and policy recommendations for improving aid worker safety in high-risk environments."
  }
]